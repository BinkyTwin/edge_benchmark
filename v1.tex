\documentclass[11pt,a4paper]{article}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{svg}  % Support for SVG figures
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=2.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{natbib}
\usepackage{url}
\usepackage{booktabs}
\usepackage{makecell}
\renewcommand\cellalign{lc}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  frame=single,
  columns=fullflexible,
  breaklines=true
}


% ============================================
% CONFIGURATION
% ============================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
}

\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}

% ============================================
% TITRE ET AUTEURS
% ============================================
\title{
\textbf{Local Generative AI on Edge Devices in Regulated Banking Environments: Benchmarking SLM Performance and Compliance Implications on Consumer Hardware}
\\[0.5em]
\large IA Générative Locale sur Appareils Edge en Environnement Bancaire Réglementé : Benchmark de Performance des SLMs et Implications de Conformité sur Matériel Grand Public
}

\author{
\makebox[\textwidth][c]{%
\begin{tabular}{c c}
\textbf{Abdelatif Djeddou}\textsuperscript{1} &
\textbf{Manissa Bouda}\textsuperscript{1} \\
\texttt{abdelatif.djeddou@edu.devinci.fr} &
\texttt{manissa.bouda@edu.devinci.fr}
\end{tabular}}\\[0.5em]
\textsuperscript{1}École de Management Léonard de Vinci
}

\date{Décembre 2025}

\begin{document}

\maketitle

% ============================================
% RÉSUMÉ
% ============================================
\begin{abstract}
Le déploiement d'intelligence artificielle générative dans le secteur bancaire fait face à des contraintes critiques : latence ultra-faible, confidentialité des données, et conformité réglementaire stricte. Les Small Language Models (SLMs), avec leurs 2-4 milliards de paramètres, émergent comme solution prometteuse pour l'inférence locale sur appareils de consommation. Cependant, aucun cadre d'évaluation n'intègre simultanément les dimensions de performance, de capacité métier et de conformité réglementaire pour le domaine bancaire.

Cette étude présente un framework de benchmark multi-dimensionnel évaluant trois familles de SLMs (Gemma 3n E4B, Qwen3-VL 4B, Ministral 3 3B) déployés sur Apple Silicon (MacBook Air M4, 16 Go RAM) via les formats MLX et GGUF Q4\_K\_M. Notre méthodologie couvre : (1) la performance système (TTFT, débit, consommation mémoire) sur trois scénarios bancaires réalistes ; (2) les capacités métier (classification d'intents Banking77, sentiment financier, génération de code) ; et (3) la conformité aux cadres NIST AI RMF 1.0 et OWASP Top 10.

Les résultats montrent des latences TTFT moyennes comprises entre 0,30 et 6,73 secondes selon le scénario et le modèle (avec un maximum observé de 10,9s pour Qwen MLX en summarization), avec des débits allant de 7,1 à 41,1 tokens/s, et un taux de JSON valide de 100\% sur l'extraction structurée. Sur Banking77 (3 076 échantillons), les SLMs atteignent 59,4--64,0\% d'accuracy en zero-shot, tandis que Financial PhraseBank (970 échantillons) se situe entre 72,7 et 73,5\%. Le benchmark HumanEval (164 problèmes) indique un Pass@1 allant de 40,2 à 57,3\% selon le modèle. La mesure mémoire rapportée correspond à la consommation RSS du processus de benchmark (et non à l'empreinte totale du runtime LM Studio). L'analyse de conformité recense 7 risques principaux et 9 contrôles, avec des recommandations concrètes, et un audit de licences opposant Apache 2.0 (Qwen, Ministral) à un régime plus restrictif (Gemma Terms of Use).

Ce travail contribue un framework open-source original intégrant performance, capacité et conformité pour l'évaluation des SLMs dans les environnements bancaires réglementés, offrant aux praticiens un outil de décision pour le déploiement edge.

\vspace{0.5em}
\noindent\textbf{Mots-clés :} Small Language Models, Edge Computing, Secteur Bancaire, Conformité Réglementaire, NIST AI RMF, Apple Silicon, MLX, Benchmarking
\end{abstract}

\newpage

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}

\subsection{Contexte et Motivation}

Le secteur des services financiers traverse une transformation numérique majeure, catalysée par l'émergence de l'intelligence artificielle générative. Les Large Language Models (LLMs) tels que GPT, Claude et Gemini ont démontré des capacités remarquables pour l'automatisation des tâches cognitives, incluant la classification d'intents clients, l'analyse de sentiment financier et la génération de rapports \citep{ibm2024genai}. Cependant, leur déploiement dans les environnements bancaires réglementés se heurte à des obstacles structurels : la latence des appels API cloud compromet l'expérience utilisateur temps réel, la transmission de données sensibles vers des infrastructures tierces soulève des préoccupations de conformité GDPR et de souveraineté des données, et les coûts opérationnels d'inférence cloud demeurent prohibitifs pour les cas d'usage à haut volume \citep{nist2023rmf}.

Dans ce contexte, les Small Language Models (SLMs) --- modèles de 2 à 4 milliards de paramètres optimisés pour l'inférence edge --- émergent comme alternative prometteuse. Les développements récents de Gemma \citep{google2024gemma}, Phi-3 \citep{microsoft2024phi3}, Qwen \citep{alibaba2024qwen} et Mistral \citep{mistral2024ministral} ont démontré que ces modèles compacts peuvent rivaliser avec leurs homologues plus volumineux sur des tâches spécifiques, tout en permettant une exécution locale sur matériel de consommation. Cette capacité d'inférence on-device répond directement aux exigences bancaires de confidentialité (aucune donnée ne quitte l'appareil), de latence (élimination de la latence réseau) et de disponibilité (fonctionnement hors-ligne) \citep{wang2024slmedge}.

Parallèlement, l'architecture Apple Silicon, avec son modèle de mémoire unifiée et le framework MLX optimisé, offre une plateforme particulièrement adaptée au déploiement SLM. Les benchmarks préliminaires suggèrent que les puces M3/M4 atteignent des performances d'inférence comparables aux GPU NVIDIA de gamme moyenne, tout en maintenant une efficacité énergétique supérieure \citep{schall2025mlx}. Le format GGUF, popularisé par llama.cpp, fournit une alternative portable bénéficiant d'un écosystème plus large.

\subsection{Problématique de Recherche}

Malgré ces avancées, la littérature présente des lacunes significatives concernant l'évaluation systématique des SLMs pour le secteur bancaire. Les études existantes se concentrent typiquement sur une dimension unique --- soit la performance système \citep{wang2024slmedge}, soit les capacités linguistiques \citep{casanueva2020banking77}, soit les considérations de sécurité \citep{zhang2025jailbreak} --- sans offrir de vision intégrée. De plus, les évaluations sur Apple Silicon demeurent anecdotiques, et aucun framework ne quantifie la conformité aux cadres réglementaires émergents tels que NIST AI RMF 1.0 \citep{nist2023rmf} ou OWASP Top 10 LLM \citep{owasp2025top10}.

Cette recherche adresse trois questions fondamentales :

\begin{enumerate}[label=\textbf{RQ\arabic*}:]
    \item \textbf{Performance Viable ?} Les SLMs quantifiés (3-4B paramètres) peuvent-ils atteindre des métriques de performance acceptables (TTFT 300--700ms \textit{très bon}, 700--1000ms \textit{bon} en interactif, débit $>$20 tokens/s, JSON valide $>$95\%) sur matériel Apple Silicon de consommation ?
    
    \item \textbf{Capacité Bancaire ?} Ces modèles démontrent-ils des capacités suffisantes pour les tâches bancaires critiques (classification d'intents, analyse de sentiment financier) en évaluation zero-shot ?
    
    \item \textbf{Risques Conformité ?} Quels risques résiduels identifie-t-on selon les frameworks NIST AI RMF et OWASP, et quelles mitigations opérationnelles peuvent être recommandées ?
\end{enumerate}

\subsection{Contributions}

Cette étude apporte les contributions suivantes :

\begin{itemize}
    \item \textbf{Framework Multi-Dimensionnel} : Nous présentons un framework de benchmark intégrant simultanément performance système, capacité métier et analyse de risques réglementaires pour l'évaluation des SLMs en contexte bancaire --- une combinaison rarement traitée dans la littérature existante.
    
    \item \textbf{Évaluation Comparative MLX vs GGUF} : Nous fournissons une comparaison systématique et reproductible des formats d'inférence MLX (Apple-natif) et GGUF sur trois familles de SLMs, quantifiant les trade-offs performance/portabilité --- un aspect peu documenté dans la littérature actuelle.
    
    \item \textbf{Benchmarks Domaine Bancaire} : Nous évaluons les capacités sur Banking77 (77 intents, 3 076 échantillons) et Financial PhraseBank (970 échantillons), établissant des baselines zero-shot pour le secteur financier.
    
    \item \textbf{Opérationnalisation NIST/OWASP} : Nous proposons une méthodologie d'analyse de risques structurée basée sur les frameworks NIST AI RMF 1.0 et OWASP Top 10 LLM, applicable aux évaluations industrielles.
    
    \item \textbf{Reproductibilité} : L'infrastructure complète (code, configurations, datasets) est publiée en open-source, incluant un système de checkpoint et de reproductibilité déterministe.
\end{itemize}

\noindent\textbf{Portée et limitations principales :} Cette étude établit des baselines zero-shot sur un matériel unique (Apple Silicon M4). Les résultats sont adaptés au triage et à la préqualification, non à la prise de décision autonome. L'analyse de risques proposée est une évaluation structurée et ne constitue pas un audit de conformité formel.

\subsection{Organisation de l'Article}

La suite de cet article est organisée comme suit. La Section~\ref{sec:travaux} présente les travaux connexes couvrant les développements SLMs, le déploiement edge et les cadres réglementaires. La Section~\ref{sec:methodologie} détaille notre méthodologie expérimentale, incluant la sélection des modèles, les protocoles de benchmark et le framework de conformité. La Section~\ref{sec:resultats} présente les résultats empiriques. La Section~\ref{sec:discussion} discute les implications, limitations et recommandations. Enfin, la Section~\ref{sec:conclusion} conclut et esquisse les directions futures.

% ============================================
% 2. TRAVAUX CONNEXES
% ============================================
\section{Travaux Connexes}
\label{sec:travaux}

\subsection{Développements des Small Language Models}

L'année 2024-2025 a marqué une inflexion dans le développement des modèles de langage compacts. Contrairement à la course aux paramètres caractérisant la période 2020-2023, les laboratoires de recherche ont réorienté leurs efforts vers l'efficacité, produisant des modèles de 2-4 milliards de paramètres optimisés pour l'inférence edge.

\textbf{Gemma} \citep{google2024gemma}, développé par Google DeepMind, constitue une famille de modèles ouverts basée sur les architectures Gemini. La variante Gemma 3n E4B intègre l'attention multi-requête (Multi-Query Attention, MQA) réduisant significativement l'empreinte mémoire lors de l'inférence. Entraînée sur 2-6 trillions de tokens, cette architecture atteint 42-45\% sur MMLU en évaluation zero-shot.

\textbf{Phi-3} \citep{microsoft2024phi3}, produit par Microsoft Research, illustre le paradigme de l'efficacité paramétrique. Grâce à une curation rigoureuse des données d'entraînement et à l'instruction-tuning, Phi-3-mini (3.8B paramètres) atteint 50-55\% sur MMLU, surpassant des modèles significativement plus volumineux.

\textbf{Qwen3} \citep{alibaba2024qwen}, développé par Alibaba DAMO Academy, offre des capacités multimodales (vision + langage) avec support de contexte étendu jusqu'à 262K tokens. La licence Apache 2.0 facilite les déploiements commerciaux.

\textbf{Mistral/Ministral} \citep{mistral2024ministral}, de Mistral AI, adopte une approche edge-first avec des modèles de 3B paramètres spécifiquement optimisés pour les appareils contraints en ressources.

La taxonomie émergente distingue trois catégories : ultra-légers (1-2B, e.g., TinyLlama), performance-optimisés (3-4B, e.g., Gemma 3n, Phi-3), et multimodaux (3-4B avec vision, e.g., Qwen3-VL) \citep{premai2025slm}.

\subsection{Déploiement Edge et Optimisations Matérielles}

Le déploiement de SLMs sur appareils de consommation nécessite des optimisations spécifiques au matériel cible.

\textbf{Apple Silicon et MLX.} Le framework MLX \citep{apple2024mlx}, développé par Apple, exploite le modèle de mémoire unifiée des puces M-series. Contrairement aux architectures GPU/CPU traditionnelles nécessitant des transferts mémoire explicites, MLX permet un dispatch automatique des opérations entre CPU et GPU sans surcharge de copie. Les benchmarks indiquent que M2 Max atteint une accélération de 5x par rapport à M1 pour l'inférence BERT-base \citep{schall2025mlx}. Les puces M3/M4 avec 8-10 cœurs GPU démontrent des performances comparables aux NVIDIA RTX 3060 pour l'inférence LLM, avec une efficacité énergétique supérieure (40-80W vs 170W).

\textbf{GGUF et llama.cpp.} Le format GGUF (GGML Unified Format), standardisé par la communauté llama.cpp \citep{llamacpp2024}, offre une portabilité cross-platform. Bien que légèrement moins performant sur Apple Silicon (10-20\% de perte vs MLX natif selon nos observations), GGUF bénéficie d'un écosystème plus large et d'une meilleure compatibilité avec les outils tiers comme LM Studio et Ollama.

\textbf{Quantification.} La compression des poids en précision réduite (INT4, INT8) constitue une technique critique pour le déploiement edge. Les schémas Q4\_K\_M et Q4\_0 réduisent l'empreinte mémoire de 4x avec une dégradation de précision typiquement inférieure à 5\% sur les benchmarks standards \citep{prieto2025edge}. Cependant, des études récentes alertent sur les risques de sécurité induits par la quantification, notamment une vulnérabilité accrue aux attaques jailbreak \citep{litelmguard2025}.

\subsection{IA dans le Secteur Bancaire}

L'application des modèles de langage au secteur financier couvre plusieurs cas d'usage critiques.

\textbf{Classification d'Intents.} Le dataset Banking77 \citep{casanueva2020banking77}, composé de 13 083 requêtes client annotées avec 77 catégories d'intents, constitue le benchmark de référence pour l'évaluation NLP bancaire. Les études récentes \citep{hassan2024banking} comparant BERT, Gemma 7B et LLaMA 8B sur ce dataset indiquent que les modèles BERT fine-tunés (e.g., SlovakBERT) surpassent les LLMs génératifs en mode zero-shot (77\% vs 48-52\% accuracy), soulignant l'importance du fine-tuning domaine pour les SLMs.

\textbf{Analyse de Sentiment Financier.} Le Financial PhraseBank \citep{malo2014fpb} offre 970 phrases annotées selon trois classes de sentiment (positif, neutre, négatif), représentant un benchmark pour l'analyse automatisée des communications financières.

\textbf{Détection de Fraude et AML.} L'apprentissage fédéré préservant la confidentialité \citep{fedrd2024} émerge comme paradigme prometteur pour l'entraînement collaboratif de modèles anti-fraude entre institutions financières sans partage de données brutes.

\subsection{Cadres de Conformité et Gestion des Risques IA}

\textbf{NIST AI Risk Management Framework 1.0.} Publié en 2023, le NIST AI RMF \citep{nist2023rmf} propose un cadre volontaire structuré autour de quatre fonctions : GOVERN (gouvernance organisationnelle), MAP (identification contextuelle), MEASURE (évaluation quantitative) et MANAGE (traitement des risques). Le profil NIST AI 600-1 \citep{nist2024genai} étend ce cadre aux spécificités des modèles génératifs, adressant les risques d'hallucination, de suivi d'instructions et de capacités émergentes.

\textbf{OWASP Top 10 LLM.} Le framework OWASP \citep{owasp2025top10} identifie les dix risques critiques pour les applications LLM : injection de prompt (LLM01), gestion non-sécurisée des sorties (LLM02), empoisonnement des données (LLM03), déni de service modèle (LLM04), vulnérabilités chaîne d'approvisionnement (LLM05), divulgation d'informations sensibles (LLM06), conception non-sécurisée de plugins (LLM07), agentivité excessive (LLM08), sur-confiance (LLM09), et vol de modèle (LLM10).

\textbf{Modèle de Maturité.} Des travaux récents \citep{fischer2024maturity} proposent un modèle de maturité à quatre niveaux (Ad Hoc, Géré, Défini, Optimisé) pour l'implémentation du NIST AI RMF, observant que la majorité des organisations financières demeurent aux niveaux 1-2.

\subsection{Sécurité des SLMs}

La vulnérabilité des SLMs aux attaques adversariales constitue une préoccupation croissante. Une étude empirique \citep{zhang2025jailbreak} évaluant 13 SLMs (Gemma, Phi, Mistral, LLaMA) révèle des taux de succès jailbreak de 65-75\%, significativement supérieurs aux LLMs plus volumineux bénéficiant d'alignements plus robustes. LiteLMGuard \citep{litelmguard2025} propose un filtrage de prompt on-device atteignant 87\% de blocage des requêtes malveillantes avec un overhead inférieur à 5ms, adressant spécifiquement les risques induits par la quantification.

\subsection{Lacunes Identifiées}

Notre revue de littérature identifie cinq lacunes principales que ce travail adresse :

\begin{enumerate}
    \item \textbf{Absence de benchmark intégré} : Aucun framework ne combine performance système, capacité métier et conformité réglementaire.
    \item \textbf{Comparaison MLX/GGUF limitée} : Les études systématiques comparant ces formats sur SLMs demeurent rares.
    \item \textbf{Évaluation bancaire insuffisante} : Les SLMs sont rarement évalués sur Banking77 et Financial PhraseBank conjointement.
    \item \textbf{NIST RMF non opérationnalisé} : Peu de méthodologies opérationnelles existent pour appliquer NIST AI RMF aux SLMs.
    \item \textbf{Reproductibilité rare} : Peu d'études fournissent une infrastructure reproductible complète.
\end{enumerate}

% ============================================
% 3. MÉTHODOLOGIE
% ============================================
\section{Méthodologie}
\label{sec:methodologie}

Cette section détaille notre méthodologie expérimentale, couvrant la sélection des modèles, l'environnement matériel, les protocoles de benchmark et le framework de conformité.

\subsection{Sélection des Modèles}

Nous évaluons trois familles de SLMs représentant l'état de l'art 2024-2025, chacune testée en deux formats d'inférence lorsque disponible :

\begin{table}[H]
\centering
\caption{Configurations de modèles évaluées}
\label{tab:modeles}
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{ID} & \textbf{Modèle} & \textbf{Paramètres} & \textbf{Format} & \textbf{Quantification} \\
\midrule
M1 & Gemma 3n E4B & 4B & MLX & 4-bit \\
M2 & Gemma 3n E4B & 4B & GGUF & Q4\_K\_M \\
M3 & Qwen3-VL 4B & 4B & MLX & 4-bit \\
M4 & Qwen3-VL 4B & 4B & GGUF & Q4\_K\_M \\
M5 & Ministral 3 3B & 3B & GGUF & Q4\_K\_M \\
\bottomrule
\end{tabular}
\end{table}

Les critères de sélection incluent : (1) taille compatible edge (3-4B paramètres), (2) disponibilité en formats MLX et/ou GGUF, (3) licence permissive (Apache 2.0, Gemma License), et (4) représentativité des architectures récentes (MQA, RoPE, vision-langage).

\subsection{Environnement Matériel}

Les expérimentations sont conduites sur la configuration suivante :

\begin{table}[H]
\centering
\caption{Spécifications matérielles de l'environnement de test}
\label{tab:hardware}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Composant} & \textbf{Spécification} \\
\midrule
Appareil & MacBook Air M4 (2024) \\
Processeur & Apple M4, 10 cœurs (6 performance + 4 efficacité) \\
Threads & 10 threads \\
Mémoire unifiée & 16 Go \\
Stockage & SSD NVMe \\
Système d'exploitation & macOS Tahoe (26.1.0) \\
Runtime & LM Studio (MLX backend), llama.cpp (GGUF backend) \\
\bottomrule
\end{tabular}
\end{table}

Cette configuration représente un laptop de consommation typique, reflétant les contraintes de déploiement en agence bancaire ou en mobilité.

\begin{table}[H]
\centering
\caption{Stack logiciel exact utilisé pour les expérimentations}
\label{tab:stack}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Composant} & \textbf{Version / Configuration} \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Runtimes d'inférence}} \\
LM Studio & 0.3.36 (Build 1) \\
llama.cpp (backend GGUF) & b7437 (commit ec98e20) \\
LM Studio MLX (backend MLX) & v0.37.0 \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Modèles téléchargés (identifiants LM Studio)}} \\
Gemma 3n E4B (MLX) & google/gemma-3n-e4b-it-mlx-4bit \\
Gemma 3n E4B (GGUF) & google/gemma-3n-e4b-it-GGUF/gemma-3n-E4B-it-Q4\_K\_M.gguf \\
Qwen3-VL 4B (MLX) & qwen/qwen3-vl-4b-instruct-mlx-4bit \\
Qwen3-VL 4B (GGUF) & qwen/qwen3-vl-4b-instruct-GGUF/qwen3-vl-4b-instruct-Q4\_K\_M.gguf \\
Ministral 3 3B (GGUF) & mistralai/ministral-3-3b-instruct-GGUF/ministral-3b-instruct-Q4\_K\_M.gguf \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Paramètres runtime}} \\
GPU layers & Auto (tous sur GPU unifié) \\
Context window & 8192 tokens (standardisé) \\
Threads CPU & 10 (6 performance + 4 efficacité) \\
Flash Attention & Activé \\
\midrule
\multicolumn{2}{@{}l@{}}{\textit{Environnement Python}} \\
Python & 3.14.0 \\
openai (client API) & 1.58.1 \\
datasets (Hugging Face) & 3.2.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Protocole d'Évaluation de Performance}

\subsubsection{Métriques}

Nous mesurons trois catégories de métriques :

\begin{itemize}
    \item \textbf{Latence} :
    \begin{itemize}
        \item \textit{TTFT (Time-to-First-Token)} : Temps entre la soumission du prompt et la génération du premier token. Références usuelles : 300--700ms (\textit{très bon}), 700--1000ms (\textit{bon}), dépendant de la longueur du prompt.
        \item \textit{Débit} : Tokens générés par seconde (tokens/s). Cible indicative : $>$20 tokens/s (scénarios interactifs) et $>$15 tokens/s (summarization).
    \end{itemize}
    \item \textbf{Mémoire} :
    \begin{itemize}
        \item \textit{RAM Peak (RSS processus)} : Pic de consommation mémoire du processus de benchmark (proxy, ne reflète pas l'empreinte totale du runtime LM Studio).
    \end{itemize}
    \item \textbf{Stabilité} :
    \begin{itemize}
        \item Écart-type des métriques sur runs répétés.
    \end{itemize}
\end{itemize}

Nous reportons également des métriques de queue de distribution (p95 et min/max). Le p99 n'est pas inclus dans cette version et pourra être ajouté lors d'une relance des runs.

\subsubsection{Scénarios de Test}

Nous définissons trois scénarios représentatifs des cas d'usage bancaires :

\begin{table}[H]
\centering
\caption{Scénarios de benchmark performance}
\label{tab:scenarios}
\begin{tabular}{@{}lL{4cm}cc@{}}
\toprule
\textbf{Scénario} & \textbf{Description} & \textbf{Tokens Entrée} & \textbf{Tokens Sortie} \\
\midrule
S1: Interactive & Réponse courte à requête client & 200-400 & 128 \\
S2: Summarization & Résumé de document financier & 2000-4000 & 256-512 \\
S3: JSON & Extraction structurée (intents, entités) & 500-1000 & 100-300 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Protocole Expérimental}

Pour assurer la validité statistique :

\begin{itemize}
    \item \textbf{Runs par scénario} : $n = 200$ (largement supérieur au seuil $n \geq 30$ pour la normalité asymptotique)
    \item \textbf{Warm-up} : 3 runs d'échauffement exclus de l'analyse pour les modèles GGUF ; 15 runs pour les modèles MLX (effets de compilation/cache plus longs)
    \item \textbf{Cool-down} : 2 secondes entre runs pour stabilisation thermique
    \item \textbf{Déterminisme} : seed = 42, temperature = 0, top\_p = 1.0 pour reproductibilité stricte
    \item \textbf{Streaming} : activé pour toutes les requêtes ; max\_tokens défini par scénario (128/512/300)
    \item \textbf{Warmup MLX} : Les effets de compilation JIT et de cache MLX rendent nécessaire une phase d'échauffement étendue (15 runs) pour stabiliser le TTFT.
\end{itemize}

\noindent\textbf{Confirmation du protocole :} Tous les benchmarks de performance utilisent exactement : warmup exclu des mesures, $n=200$ runs comptabilisés, seed=42, temperature=0, top\_p=1.0, max\_tokens variable selon le scénario.

\subsubsection{Analyse Statistique}

Les métriques sont analysées avec :

\begin{itemize}
    \item \textbf{Statistiques descriptives} : Moyenne, médiane, écart-type, min/max
    \item \textbf{Intervalles de confiance} : Bootstrap 95\% (10 000 itérations)
    $$CI = \left[\text{percentile}_{2.5\%}(\hat{\theta}), \text{percentile}_{97.5\%}(\hat{\theta})\right]$$
    \item \textbf{Tests de comparaison} : t-test apparié (distributions normales), test de Wilcoxon (distributions non-normales)
    \item \textbf{Correction comparaisons multiples} : Correction de Holm pour contrôle FWER
    \item \textbf{Tailles d'effet} : $d$ de Cohen pour interprétation pratique 
    $$d = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}}}$$
Seuils : small (0.2), medium (0.5), large (0.8).
\end{itemize}

\subsection{Protocole d'Évaluation des Capacités}

\subsubsection{Tâches et Datasets}

Nous évaluons cinq dimensions de capacité :

\begin{table}[H]
\centering
\caption{Tâches d'évaluation des capacités}
\label{tab:capacite}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Tâche} & \textbf{Dataset} & \textbf{Échantillons} & \textbf{Métrique} \\
\midrule
Classification d'intents & Banking77 (test) & 3 076 & Accuracy, F1 Macro \\
Sentiment financier & Financial PhraseBank (test) & 970 & Accuracy, F1 Macro \\
Génération de code & HumanEval (complet) & 164 & Pass@1, taux syntaxe valide \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Protocole Zero-Shot}

Toutes les évaluations sont conduites en mode \textbf{zero-shot} sans exemples in-context, reflétant la contrainte de déploiement réel où le fine-tuning n'est pas toujours praticable. Cette approche établit une baseline conservative des capacités out-of-the-box.

Les prompts sont standardisés selon les templates suivants :

\begin{lstlisting}[caption={Intent classification prompt template},label={lst:prompt-intent}]
System: You are a banking assistant. Follow the instructions exactly.

User: Classify the following banking query into exactly one of the 77 intent categories.
Return only the category label.

Query: {{query}}

Assistant:
\end{lstlisting}

\begin{lstlisting}[caption={Financial sentiment prompt template},label={lst:prompt-sentiment}]
System: You are a banking assistant. Follow the instructions exactly.

User: Analyze the sentiment of the following financial statement.
Choose one label from: positive, neutral, negative.
Return only the label.

Statement: {{statement}}

Assistant:
\end{lstlisting}



\subsection{Framework d'Évaluation de Conformité}

\subsubsection{Mapping NIST AI RMF 1.0}

Nous opérationnalisons les quatre fonctions NIST en critères évaluables pour les SLMs :

\begin{table}[H]
\centering
\caption{Mapping NIST AI RMF pour SLMs}
\label{tab:nist}
\begin{tabular}{@{}lL{6cm}l@{}}
\toprule
\textbf{Fonction} & \textbf{Critères Évalués} & \textbf{Évidence} \\
\midrule
GOVERN & Documentation licence, traçabilité origine, politique usage & Registre modèles, conformité licence \\
MAP & Identification risques domaine, caractérisation usage & Threat model, périmètre d'usage \\
MEASURE & Tests biais, robustesse, vulnérabilités & Résultats benchmarks, audits \\
MANAGE & Mitigations disponibles, monitoring intégrable & Contrôles, procédures d'incident \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Checklist OWASP Top 10 LLM}

Chaque risque OWASP est évalué selon une échelle de sévérité et de mitigation :

\begin{table}[H]
\centering
\caption{Grille d'évaluation OWASP Top 10 LLM}
\label{tab:owasp}
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{ID} & \textbf{Risque} & \textbf{Sévérité Banking} & \textbf{Mitigation Disponible} \\
\midrule
LLM01 & Injection de Prompt & Critique & Partielle \\
LLM02 & Gestion Non-Sécurisée Sorties & Élevée & Oui \\
LLM03 & Empoisonnement Données & Moyenne & N/A (pré-entraîné) \\
LLM04 & Déni de Service Modèle & Moyenne & Oui \\
LLM05 & Vulnérabilités Chaîne & Élevée & Audit requis \\
LLM06 & Divulgation Informations & Critique & Partielle \\
LLM07 & Plugins Non-Sécurisés & Moyenne & N/A (sans plugins) \\
LLM08 & Agentivité Excessive & Critique & Configuration \\
LLM09 & Sur-Confiance & Critique & Formation utilisateurs \\
LLM10 & Vol de Modèle & Élevée & Contrôle accès \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Audit de Licences}

La compatibilité commerciale est évaluée selon :
\begin{itemize}
    \item Type de licence (Apache 2.0, MIT, Gemma License, etc.)
    \item Restrictions usage commercial
    \item Obligations de divulgation
    \item Compatibilité avec déploiement banking
\end{itemize}

\subsection{Infrastructure de Reproductibilité}

Pour garantir la reproductibilité :

\begin{itemize}
    \item \textbf{Exécution déterministe} : Seed fixe pour tous les générateurs aléatoires
    \item \textbf{Hachage configuration} : SHA256 des fichiers YAML de configuration
    \item \textbf{Capture environnement} : Versions OS, runtime, dépendances
    \item \textbf{Système de checkpoint} : Sauvegarde incrémentale permettant reprise après interruption
    \item \textbf{Logging détaillé} : JSONL par run avec timestamps et métriques
\end{itemize}

Le code source, les configurations et les datasets sont disponibles sur GitHub\footnote{\url{https://github.com/BinkyTwin/edge_benchmark}}.

% ============================================
% 4. RÉSULTATS
% ============================================
\newpage
\section{Résultats}
\label{sec:resultats}

Cette section présente les résultats empiriques de notre évaluation multi-dimensionnelle.

\subsection{Performance Système}

\subsubsection{Time-to-First-Token (TTFT)}

Le Tableau~\ref{tab:ttft} présente les résultats TTFT par modèle et scénario.

\begin{table}[H]
\centering
\caption{Résultats TTFT (ms) par modèle et scénario. Moyenne $\pm$ écart-type sur 200 runs. IC 95\% bootstrap entre parenthèses.}
\label{tab:ttft}
\scriptsize
\setlength{\tabcolsep}{5pt}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Modèle} & \textbf{S1: Interactive} & \textbf{S2: Summarization} & \textbf{S3: JSON} \\
\midrule

Gemma 3n E4B (MLX) &
\makecell[l]{737 $\pm$ 278\\(701--777)\\p95=1372, min=538, max=1885} &
\makecell[l]{2865 $\pm$ 317\\(2822--2910)\\p95=3412, min=2462, max=4382} &
\makecell[l]{2139 $\pm$ 1692\\(1981--2407)\\p95=2560, min=1690, max=25111} \\

Gemma 3n E4B (GGUF) &
\makecell[l]{670 $\pm$ 60\\(662--678)\\p95=777, min=556, max=877} &
\makecell[l]{638 $\pm$ 360\\(607--693)\\p95=749, min=512, max=5635} &
\makecell[l]{2086 $\pm$ 303\\(2047--2130)\\p95=2582, min=1684, max=4632} \\

Qwen3-VL 4B (MLX) &
\makecell[l]{2629 $\pm$ 422\\(2572--2688)\\p95=3417, min=2134, max=3982} &
\makecell[l]{6732 $\pm$ 1266\\(6559--6908)\\p95=9348, min=5086, max=10897} &
\makecell[l]{3762 $\pm$ 1468\\(3621--3992)\\p95=4530, min=3296, max=23606} \\

Qwen3-VL 4B (GGUF) &
\makecell[l]{\textbf{443 $\pm$ 56}\\(435--450)\\p95=537, min=345, max=635} &
\makecell[l]{475 $\pm$ 319\\(447--525)\\p95=552, min=387, max=4868} &
\makecell[l]{1104 $\pm$ 77\\(1093--1114)\\p95=1220, min=980, max=1471} \\

Ministral 3 3B (GGUF) &
\makecell[l]{\textbf{440 $\pm$ 67}\\(431--450)\\p95=558, min=349, max=742} &
\makecell[l]{\textbf{296 $\pm$ 180}\\(278--324)\\p95=381, min=215, max=2731} &
\makecell[l]{\textbf{978 $\pm$ 105}\\(965--993)\\p95=1176, min=854, max=1827} \\

\bottomrule
\end{tabular}
\end{table}


% ============================================
% PLACEHOLDER FIGURE 1
% ============================================
\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{figure_1_ttft}
\caption{Comparaison du Time-to-First-Token (TTFT) par modèle et scénario. Les références usuelles situent le TTFT \textit{très bon} entre 300--700ms et \textit{bon} entre 700--1000ms, en particulier pour les prompts courts (S1).}
\label{fig:ttft}
\end{figure}

\textbf{Observations clés :}
\begin{itemize}
    \item Le TTFT dépend fortement de la taille du prompt : la comparaison pertinente s'effectue principalement à scénario équivalent (S1 vs S1, etc.).
    \item Les formats GGUF démontrent des latences TTFT inférieures aux formats MLX pour Gemma et Qwen, suggérant une meilleure efficacité du prefill côté llama.cpp.
    \item Qwen3-VL 4B (GGUF) et Ministral 3 3B (GGUF) passent sous 500ms en S1 (443ms et 440ms), et Gemma GGUF reste dans la zone \textit{très bon} (670ms).
    \item Les modèles MLX présentent des TTFT nettement plus élevés (0.74--2.63s en S1), avec une pénalité marquée pour Qwen3-VL.
    \item Le scénario S2 (Summarization) amplifie ces différences, avec TTFT MLX entre 2.9s et 6.7s.
    \item Attention à l'effet de warmup/compilation/cache côté MLX : des runs insuffisamment stabilisés peuvent biaiser le TTFT à la hausse ou à la baisse. Les mesures sont comparables uniquement si les conditions de warmup sont strictement contrôlées.
\end{itemize}

\subsubsection{Débit de Génération}

Le Tableau~\ref{tab:throughput} présente les débits mesurés en tokens par seconde.

\begin{table}[H]
\centering
\caption{Débit de génération (tokens/s) par modèle et scénario. Moyenne $\pm$ écart-type sur 200 runs (p95 entre parenthèses).}
\label{tab:throughput}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Modèle} & \textbf{S1: Interactive} & \textbf{S2: Summarization} & \textbf{S3: JSON} \\
\midrule
Gemma 3n E4B (MLX) & 36.81 $\pm$ 1.40 (38.62) & \textbf{39.05 $\pm$ 1.55 (40.79)} & 14.70 $\pm$ 0.78 (15.61) \\
Gemma 3n E4B (GGUF) & 26.38 $\pm$ 2.16 (29.24) & 26.41 $\pm$ 3.16 (29.48) & 7.06 $\pm$ 2.05 (10.58) \\
Qwen3-VL 4B (MLX) & 39.69 $\pm$ 6.91 (50.66) & 30.31 $\pm$ 3.40 (36.29) & \textbf{19.60 $\pm$ 1.85 (21.74)} \\
Qwen3-VL 4B (GGUF) & 37.98 $\pm$ 3.48 (43.58) & 25.37 $\pm$ 4.54 (33.05) & 14.87 $\pm$ 4.29 (20.73) \\
Ministral 3 3B (GGUF) & \textbf{41.11 $\pm$ 6.17 (50.84)} & 27.44 $\pm$ 2.95 (32.94) & 15.81 $\pm$ 3.97 (21.06) \\
\bottomrule
\end{tabular}
\end{table}

% ============================================
% PLACEHOLDER FIGURE 2
% ============================================
\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{figure_2_throughput.svg}
\caption{Débit de génération par modèle et scénario. Les performances restent majoritairement au-dessus de 20 tokens/s, mais chutent sur l'extraction JSON.}
\label{fig:throughput}
\end{figure}

\textbf{Observations clés :}
\begin{itemize}
    \item Les débits varient de 7.06 à 41.11 tokens/s selon le scénario, avec une baisse marquée sur l'extraction JSON.
    \item L'avantage MLX sur GGUF est visible pour Gemma (S2) et Qwen (S3), mais l'écart dépend fortement du scénario.
    \item En scénario interactif, Ministral (GGUF) et Qwen (MLX/GGUF) dépassent 37 tokens/s, tandis que Gemma GGUF reste plus bas.
\end{itemize}

\subsubsection{Validité JSON (S3)}

Pour le scénario d'extraction structurée, tous les modèles atteignent un taux de JSON valide de 100\% (200/200 sorties par modèle), indiquant une forte fiabilité de format dans cette configuration.

\subsubsection{Consommation Mémoire}

Le Tableau~\ref{tab:memory} présente l'empreinte mémoire des différentes configurations.

\begin{table}[H]
\centering
\caption{Consommation mémoire (RSS du processus de benchmark)}
\label{tab:memory}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Modèle} & \textbf{Peak RSS (MB)} & \textbf{RSS moyen (MB)} \\
\midrule
Gemma 3n E4B (MLX) & 52.75 & 25.66 \\
Gemma 3n E4B (GGUF) & 27.25 & \textbf{21.29} \\
Qwen3-VL 4B (MLX) & 133.33 & 25.67 \\
Qwen3-VL 4B (GGUF) & 54.94 & 33.38 \\
Ministral 3 3B (GGUF) & 130.53 & 29.67 \\
\bottomrule
\end{tabular}
\end{table}

Ces mesures reflètent la mémoire RSS du processus Python utilisé pour le benchmark. Elles ne capturent pas l'empreinte totale du runtime LM Studio (modèle chargé, backend), et ne permettent donc pas d'inférer directement la consommation mémoire globale.

\subsection{Capacités Métier}

Les évaluations de capacité sont reportées au niveau modèle. Dans cette version, les tâches Banking77/FPB/HumanEval ont été exécutées en GGUF, ce qui limite l'analyse de l'impact du format sur la qualité.

\subsubsection{Classification d'Intents Bancaires (Banking77)}

Le Tableau~\ref{tab:banking77} présente les performances sur Banking77 en évaluation zero-shot.

\begin{table}[H]
\centering
\caption{Performance sur Banking77 (77 classes, 3 076 échantillons) en mode zero-shot}
\label{tab:banking77}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Modèle} & \textbf{Accuracy (\%)} & \textbf{F1 Macro (\%)} & \textbf{F1 Weighted (\%)} \\
\midrule
Gemma 3n E4B & 63.4 & 60.5 & 61.3 \\
Qwen3-VL 4B & \textbf{64.0} & \textbf{61.3} & \textbf{62.1} \\
Ministral 3 3B & 59.4 & 56.3 & 56.3 \\
\midrule
\textit{Référence : Gemma 7B (zero-shot)*} & \textit{48.0} & \textit{--} & \textit{--} \\
\textit{Référence : SlovakBERT (fine-tuned)*} & \textit{77.0} & \textit{75.0} & \textit{--} \\
\bottomrule
\end{tabular}
\footnotesize{*Références issues de \citet{hassan2024banking}}
\end{table}

\textbf{Observations clés :}
\begin{itemize}
    \item Les SLMs 3-4B en zero-shot atteignent 59.4--64.0\% d'accuracy, au-dessus des résultats zero-shot rapportés pour Gemma 7B (48\%).
    \item L'écart avec les modèles fine-tunés (77\%) confirme l'importance d'une adaptation domaine pour un usage production.
    \item Les résultats sont reportés au niveau modèle (format d'inférence non distingué dans l'évaluation Banking77).
\end{itemize}

\subsubsection{Sentiment Financier (Financial PhraseBank)}

Le Tableau~\ref{tab:fpb} présente les performances sur le dataset Financial PhraseBank.

\begin{table}[H]
\centering
\caption{Performance sur Financial PhraseBank (3 classes, 970 échantillons) en mode zero-shot}
\label{tab:fpb}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Modèle} & \textbf{Accuracy (\%)} & \textbf{F1 Macro (\%)} & \textbf{F1 Weighted (\%)} \\
\midrule
Gemma 3n E4B & 72.7 & 73.9 & 72.8 \\
Qwen3-VL 4B & \textbf{73.5} & \textbf{75.5} & \textbf{73.6} \\
Ministral 3 3B & 72.7 & 72.6 & 73.0 \\
\bottomrule
\end{tabular}
\end{table}

Qwen3-VL 4B obtient les meilleures performances sur cette tâche, mais les écarts restent modestes entre modèles.

\subsubsection{Génération de Code (HumanEval)}

L'évaluation HumanEval utilise le benchmark complet (164 problèmes de programmation Python). Le protocole de parsing extrait le code généré entre les balises \texttt{```python} et \texttt{```}, puis vérifie (1) la validité syntaxique via \texttt{ast.parse()}, et (2) la correction fonctionnelle via exécution des tests unitaires fournis. L'utilisation de temperature=0 garantit une génération déterministe, permettant une reproductibilité stricte --- au prix d'une exploration réduite de l'espace des solutions (ce qui peut sous-estimer le Pass@k pour $k>1$).

\begin{table}[H]
\centering
\caption{Performance sur HumanEval (164 problèmes, temperature=0)}
\label{tab:humaneval}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Modèle} & \textbf{Pass@1 (\%)} & \textbf{Syntaxe valide (\%)} \\
\midrule
Gemma 3n E4B & 40.2 & 76.2 \\
Qwen3-VL 4B & \textbf{57.3} & \textbf{87.8} \\
Ministral 3 3B & 43.3 & 87.8 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Trade-offs Performance vs Capacité}

% ============================================
% PLACEHOLDER FIGURE 4
% ============================================
\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{figure_4_tradeoff.svg}
\caption{Trade-off entre débit d'inférence et accuracy sur Banking77. Qwen3-VL 4B combine la meilleure accuracy, mais l'avantage performance dépend du format d'inférence.}
\label{fig:tradeoff}
\end{figure}

L'analyse des trade-offs montre que Qwen3-VL 4B obtient la meilleure accuracy sur Banking77, tandis que les différences de débit sont surtout sensibles entre MLX et GGUF. La sélection optimale dépend donc du compromis TTFT (GGUF) versus throughput (MLX).

\subsection{Analyse Structurée de Risques}

\textbf{Note méthodologique importante :} Cette section présente une \textit{analyse structurée de risques}, et non un audit de conformité formel. Nous appliquons les frameworks NIST AI RMF 1.0 et OWASP Top 10 LLM comme grilles d'évaluation pour identifier les risques potentiels et proposer des contrôles. Cette analyse ne constitue pas une certification de conformité et ne remplace pas un audit réglementaire conduit par des autorités compétentes.

\subsubsection{Méthodologie d'Analyse des Risques}

L'analyse repose sur le modèle NIST AI RMF 1.0, enrichi par les catégories OWASP Top 10 LLM. Chaque risque est évalué selon deux niveaux :
\begin{itemize}
    \item \textbf{Risque inhérent} : niveau de risque avant application de contrôles (High/Medium/Low)
    \item \textbf{Risque résiduel} : niveau après application des contrôles identifiés
\end{itemize}

Le rapport identifie 7 risques principaux et 9 contrôles, avec une distribution résiduelle dominée par le niveau \textit{low} (6/7). Le seul risque résiduel \textit{medium} concerne l'injection de prompt (LLM01), car les techniques de mitigation actuelles (validation d'entrée, filtrage) ne garantissent pas une protection complète contre les attaques adversariales sophistiquées.

\begin{table}[H]
\centering
\caption{Synthèse des risques (inherent $\rightarrow$ résiduel)}
\label{tab:risks}
\begin{tabular}{@{}lL{5cm}cc@{}}
\toprule
\textbf{ID} & \textbf{Catégorie} & \textbf{Inhérent} & \textbf{Résiduel} \\
\midrule
R01 & LLM01: Prompt Injection & High & Medium \\
R02 & LLM06: Sensitive Info Disclosure & High & Low \\
R03 & Device Loss/Theft & Medium & Low \\
R04 & LLM05: Supply Chain Vulnerabilities & Medium & Low \\
R05 & Licensing/Compliance & Medium & Low \\
R06 & LLM09: Overreliance & Medium & Low \\
R07 & Data Exfiltration & Medium & Low \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Résumé des contrôles :} 9 contrôles identifiés, dont 3 déjà implémentés (ex: chiffrement disque, contrôle d'accès), et 6 planifiés (validation d'entrées, registre de modèles, politique de logs).

\textbf{Recommandations clés :}
\begin{itemize}
    \item Implémenter une validation des prompts avant inférence pour réduire le risque R01.
    \item Formaliser un registre de modèles avec vérification d'intégrité (supply chain).
    \item Définir une politique de logging minimisant la rétention de données sensibles.
    \item Former les utilisateurs afin d'éviter la sur-dépendance aux réponses (R06).
\end{itemize}

\subsubsection{Compatibilité Licences}

\begin{table}[H]
\centering
\caption{Audit des licences pour déploiement bancaire commercial}
\label{tab:licences}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Modèle} & \textbf{Licence} & \textbf{Commercial} & \textbf{Restrictions} \\
\midrule
Gemma 3n E4B & Gemma Terms of Use & \checkmark (restreint) & Usages interdits et attribution requise \\
Qwen3-VL 4B & Apache 2.0 & \checkmark & Aucune \\
Ministral 3 3B & Apache 2.0 & \checkmark & Aucune \\
\bottomrule
\end{tabular}
\end{table}

Qwen3-VL 4B et Ministral 3 3B offrent la meilleure flexibilité commerciale avec Apache 2.0. Gemma autorise l'usage commercial mais impose des restrictions spécifiques et une revue d'usage pour les cas bancaires.

\subsection{Synthèse Comparative}

Nous distinguons la synthèse performance (par format d'inférence) et la synthèse capacité (au niveau modèle, le format d'inférence n'étant pas distingué dans les évaluations Banking/FPB/HumanEval).

\begin{table}[H]
\centering
\caption{Synthèse performance par format (scénario interactif S1)}
\label{tab:synthese_perf}
\begin{tabular}{@{}lC{1.8cm}C{1.8cm}C{1.8cm}C{1.8cm}C{1.8cm}@{}}
\toprule
\textbf{Dimension} & \textbf{Gemma MLX} & \textbf{Gemma GGUF} & \textbf{Qwen MLX} & \textbf{Qwen GGUF} & \textbf{Ministral} \\
\midrule
TTFT S1 (ms) & 737 & 670 & 2629 & \textbf{443} & 440 \\
Débit S1 (t/s) & 36.81 & 26.38 & 39.69 & 37.98 & \textbf{41.11} \\
JSON valide (S3) & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
Peak RSS (MB) & 52.75 & \textbf{27.25} & 133.33 & 54.94 & 130.53 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Synthèse capacité au niveau modèle}
\label{tab:synthese_cap}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dimension} & \textbf{Gemma 3n E4B} & \textbf{Qwen3-VL 4B} & \textbf{Ministral 3 3B} \\
\midrule
Banking77 Accuracy (\%) & 63.4 & \textbf{64.0} & 59.4 \\
FPB Accuracy (\%) & 72.7 & \textbf{73.5} & 72.7 \\
HumanEval Pass@1 (\%) & 40.2 & \textbf{57.3} & 43.3 \\
Licence & Gemma Terms (restreint) & Apache 2.0 & Apache 2.0 \\
\bottomrule
\end{tabular}
\end{table}

% ============================================
% 5. DISCUSSION
% ============================================
\section{Discussion}
\label{sec:discussion}

\subsection{Réponse aux Questions de Recherche}

\textbf{RQ1: Performance Viable ?}

Nos résultats démontrent que les SLMs quantifiés (3-4B paramètres) atteignent des performances système viables pour le déploiement bancaire edge sur Apple Silicon M4 :

\begin{itemize}
    \item \textbf{TTFT} : En scénario interactif, Qwen3-VL 4B (GGUF) et Ministral (GGUF) sont dans la zone \textit{très bon} (443ms et 440ms), Gemma GGUF reste \textit{très bon} (670ms), tandis que Gemma MLX et Qwen MLX restent au-dessus (737ms à 2.63s). Les scénarios longs montent jusqu'à 6.73s.
    
    \item \textbf{Débit} : Les débits interactifs sont compris entre 26 et 41 tokens/s, tandis que l'extraction JSON chute à 7--20 tokens/s, en deçà des seuils de confort pour certaines configurations.
    
    \item \textbf{Mémoire} : Les mesures disponibles concernent la RSS du processus de benchmark (26--64 MB) et ne reflètent pas l'empreinte totale du runtime LM Studio ; la compatibilité mémoire globale reste donc à confirmer.
\end{itemize}

Le format MLX conserve un avantage en throughput pour Gemma et Qwen, tandis que GGUF est plus performant en TTFT. La sélection dépend donc du compromis latence initiale (GGUF) versus débit soutenu (MLX).

\textbf{RQ2: Capacité Bancaire ?}

L'évaluation zero-shot révèle des capacités prometteuses mais avec des nuances :

\begin{itemize}
    \item \textbf{Classification d'intents} : Les 59--64\% d'accuracy sur Banking77 dépassent les résultats zero-shot rapportés pour Gemma 7B (48\%), mais restent inférieurs aux modèles fine-tunés (77\%). L'écart confirme la nécessité d'une adaptation domaine pour la production.
    
    \item \textbf{Sentiment financier} : Les 72.7--73.5\% d'accuracy sur Financial PhraseBank indiquent une compréhension raisonnable du langage financier, suffisante pour un préfiltrage automatisé avec validation humaine.
    
    \item \textbf{Génération de code} : HumanEval met en évidence des écarts importants (Pass@1 de 40.2 à 57.3\%), suggérant que la robustesse en programmation varie fortement selon les architectures.
\end{itemize}

Ces résultats suggèrent que les SLMs peuvent servir de première ligne de traitement (triage, préclassification) dans un pipeline hybride avec escalade vers des modèles plus puissants ou des opérateurs humains pour les cas complexes.

\textbf{RQ3: Risques Conformité ?}

L'analyse NIST/OWASP identifie plusieurs risques résiduels critiques :

\begin{itemize}
    \item \textbf{Injection de prompt (LLM01)} : Seul risque résiduel \textit{medium} dans l'analyse, nécessitant une validation stricte des entrées.
    
    \item \textbf{Divulgation d'informations (LLM06) et sur-dépendance (LLM09)} : Risques ramenés à \textit{low} via contrôle des sorties et human-in-loop, mais restant critiques en contexte bancaire.
    
    \item \textbf{Chaîne d'approvisionnement et conformité licence} : La gestion des sources de modèles et des restrictions d'usage doit être formalisée pour éviter les non-conformités.
\end{itemize}

Le rapport de conformité met en évidence 7 risques principaux et 9 contrôles (3 implémentés, 6 planifiés), suggérant un niveau de maturité en transition vers une gouvernance plus formalisée.

\subsection{Ce que nos résultats ne prouvent pas}

\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}
\textbf{Avertissement d'interprétation :} Cette étude établit des \textit{baselines} et identifie des tendances, mais ne constitue pas :
\begin{itemize}
    \item \textbf{Une certification de conformité} : L'analyse de risques NIST/OWASP est une évaluation structurée, non un audit formel par une autorité compétente.
    \item \textbf{Une validation production-ready} : Les résultats zero-shot (59--64\% Banking77) sont adaptés au triage/préqualification, non à la prise de décision autonome.
    \item \textbf{Une garantie de sécurité} : Les contrôles listés n'ont pas été testés empiriquement contre des attaques adversariales.
    \item \textbf{Une mesure d'empreinte mémoire complète} : La RAM reportée (RSS processus) ne capture pas l'empreinte totale du runtime LM Studio.
    \item \textbf{Une évaluation exhaustive} : Trois familles de modèles sur un seul matériel (M4) ; la généralisation requiert des tests complémentaires.
\end{itemize}
\end{minipage}}
\end{center}

\subsection{Recommandations Pratiques}

Basés sur nos résultats, nous formulons les recommandations suivantes pour les institutions financières :

\begin{enumerate}
    \item \textbf{Sélection de modèle} :
    \begin{itemize}
        \item \textit{Priorité latence initiale (interactive)} : Qwen3-VL 4B (GGUF) avec TTFT $<$500ms
        \item \textit{Priorité débit soutenu} : Qwen3-VL 4B (MLX)
        \item \textit{Priorité portabilité} : Ministral 3 3B (GGUF)
        \item \textit{Priorité licence} : Qwen3-VL 4B ou Ministral 3 3B (Apache 2.0)
    \end{itemize}
    
    \item \textbf{Architecture de déploiement} :
    \begin{itemize}
        \item Implémenter un filtrage d'entrée robuste (LiteLMGuard ou équivalent)
        \item Configurer des guardrails de sortie (validation de format, détection d'hallucination)
        \item Établir un workflow human-in-loop pour les décisions à impact
        \item Limiter les actions autorisées (lecture seule, pas d'exécution de transactions)
    \end{itemize}
    
    \item \textbf{Feuille de route conformité} :
    \begin{itemize}
        \item Documenter les cas d'usage et les risques identifiés (MAP)
        \item Établir des métriques de monitoring continues (MEASURE)
        \item Définir des procédures d'incident et de rollback (MANAGE)
        \item Former les équipes aux limitations des SLMs (GOVERN)
    \end{itemize}
    
    \item \textbf{Fine-tuning optionnel} :
    \begin{itemize}
        \item Considérer le fine-tuning LoRA/QLoRA sur Banking77 + données internes
        \item Gains attendus : +20-25 points accuracy sur classification d'intents
        \item Attention aux risques de mémorisation de données sensibles
    \end{itemize}
\end{enumerate}

\subsection{Implications pour la Recherche}

\textbf{Contribution à la littérature.} Ce travail comble plusieurs lacunes identifiées :

\begin{itemize}
    \item Un framework intégrant performance, capacité et analyse de risques pour SLMs bancaires --- combinaison rarement traitée
    \item Une comparaison systématique MLX vs GGUF avec rigueur statistique --- aspect peu documenté
    \item Une opérationnalisation du NIST AI RMF pour l'évaluation de modèles
\end{itemize}

\textbf{Implications théoriques.} Nos résultats suggèrent que l'efficacité des SLMs pour les tâches domaine-spécifiques dépend moins de la taille du modèle que de (1) l'architecture (MQA améliore l'efficacité sans sacrifier la capacité), (2) la qualité des données d'entraînement, et (3) l'optimisation du format d'inférence pour le matériel cible.

\textbf{Pistes de recherche future.} Plusieurs directions méritent exploration :

\begin{itemize}
    \item \textbf{Fine-tuning préservant la vie privée} : Application de l'apprentissage fédéré et de la confidentialité différentielle pour l'adaptation domaine
    \item \textbf{Architectures hybrides SLM-LLM} : Routage intelligent entre modèles locaux et cloud selon la complexité
    \item \textbf{Continual learning edge} : Mise à jour incrémentale des modèles sans redéploiement complet
    \item \textbf{Benchmarks adversariaux banking} : Développement de jeux de test spécifiques aux attaques financières
\end{itemize}

\subsection{Limitations}

Cette étude présente plusieurs limitations à considérer :

\begin{enumerate}
    \item \textbf{Matériel unique} : Les résultats sont obtenus sur Apple Silicon M4 ; la généralisation à d'autres plateformes (NVIDIA Jetson, x86+GPU) nécessite validation.
    
    \item \textbf{Évaluation zero-shot} : Les performances fine-tuned, probablement supérieures, ne sont pas mesurées.
    
    \item \textbf{Datasets publics} : Banking77 et FPB, bien que standards, peuvent ne pas refléter la diversité des cas d'usage internes des institutions.
    
    \item \textbf{Format d'inférence non tracé} : Les évaluations de capacités ne distinguent pas MLX vs GGUF, limitant l'analyse de l'impact du format sur la qualité.

    \item \textbf{Taille d'échantillon} : Les 200 runs par scénario, bien que statistiquement robustes, peuvent masquer des variations très rares (tail events).

    \item \textbf{Conformité non-exhaustive} : Seuls NIST et OWASP sont évalués ; les régulations spécifiques (PSD2, MiFID II) ne sont pas couvertes.

    \item \textbf{Mémoire partielle} : Les mesures RAM reflètent la RSS du processus de benchmark, pas l'empreinte totale du runtime LM Studio.
    
    \item \textbf{Queues de distribution} : Le p95 est reporté, mais le p99 n'est pas mesuré dans cette version.
    
    \item \textbf{Sécurité non-testée empiriquement} : Les contrôles listés proviennent d'une analyse de risque et non de tests d'attaque directs sur nos configurations.
\end{enumerate}

% ============================================
% 6. CONCLUSION
% ============================================
\section{Conclusion}
\label{sec:conclusion}

Cette étude a présenté une évaluation multi-dimensionnelle des Small Language Models pour le déploiement edge dans les environnements bancaires réglementés. Notre framework intégrant performance système, capacité métier et conformité réglementaire répond à un besoin critique de l'industrie financière face à l'adoption de l'IA générative locale.

\subsection{Synthèse des Contributions}

Nos expérimentations sur Apple Silicon M4 avec cinq configurations de modèles (Gemma 3n E4B, Qwen3-VL 4B, Ministral 3 3B) en formats MLX et GGUF démontrent que :

\begin{enumerate}
    \item Les SLMs quantifiés atteignent des \textbf{performances système hétérogènes} : TTFT moyen de 0.30 à 6.73s selon le scénario (maximum observé : 10.9s pour Qwen MLX en summarization), débits de 7.1 à 41.1 tokens/s, et JSON valide à 100\% sur extraction structurée (la mémoire reportée étant limitée à la RSS du processus de benchmark).
    
    \item Les \textbf{capacités métier zero-shot} (59--64\% Banking77, 72.7--73.5\% FPB) établissent des baselines solides, tandis que la \textbf{génération de code} varie fortement (Pass@1 de 40.2 à 57.3\% sur HumanEval).
    
    \item Les \textbf{risques conformité} identifiés (prompt injection, divulgation, sur-dépendance, chaîne d'approvisionnement) sont adressables via des contrôles opérationnels, avec 7 risques et 9 contrôles recensés.
    
    \item Le format \textbf{GGUF réduit la latence TTFT} tandis que \textbf{MLX favorise le débit}, imposant un compromis explicite selon les cas d'usage.
\end{enumerate}

\subsection{Implications Pratiques}

Pour les institutions financières envisageant le déploiement de SLMs :

\begin{itemize}
    \item \textbf{Le déploiement edge est techniquement viable} sur matériel de consommation actuel, avec des performances variables selon le scénario et le format d'inférence.
    
    \item \textbf{L'approche prudente recommandée} combine : SLM pour prétraitement/triage, human-in-loop pour décisions à impact, LLM cloud pour cas complexes.
    
    \item \textbf{La conformité n'est pas optionnelle} : L'intégration de guardrails, monitoring et documentation est impérative avant tout déploiement production.
\end{itemize}

\subsection{Limitations Clés}

Trois limitations majeures doivent être considérées :
\begin{itemize}
    \item \textbf{Généralisation matérielle :} Les résultats sont obtenus sur Apple Silicon M4 uniquement ; la transposition à d'autres plateformes (NVIDIA Jetson, x86+GPU, mobile) nécessite validation.
    \item \textbf{Scope de l'évaluation zero-shot :} Les performances mesurées (59--64\% Banking77) sont adaptées au triage, non à la décision autonome ; le fine-tuning domaine reste essentiel pour la production.
    \item \textbf{Nature de l'analyse de risques :} La méthodologie NIST/OWASP appliquée est une évaluation structurée, non un audit de conformité formel par une autorité compétente.
\end{itemize}

\subsection{Travaux Futurs}

Les directions prioritaires incluent :

\begin{enumerate}
    \item Extension du benchmark à d'autres plateformes edge (Windows ARM, Linux x86+GPU, mobile)
    \item Évaluation des techniques de fine-tuning préservant la vie privée (LoRA + confidentialité différentielle)
    \item Développement de benchmarks adversariaux spécifiques au domaine financier
    \item Étude longitudinale des performances en déploiement production
\end{enumerate}

\subsection{Reproductibilité}

L'ensemble du code source, des configurations et des datasets utilisés dans cette étude sont disponibles en open-source sur GitHub\footnote{\url{https://github.com/BinkyTwin/edge_benchmark}}. Nous encourageons la communauté à reproduire, étendre et améliorer ce travail.


\newpage
% ============================================
% RÉFÉRENCES
% ============================================
\bibliographystyle{plainnat}

\begin{thebibliography}{99}

\bibitem[Casanueva et al.(2020)]{casanueva2020banking77}
Casanueva, I., Temčinas, T., Gerber, D., Henderson, M., et Vulić, I. (2020).
\newblock Efficient Intent Detection with Dual Sentence Encoders.
\newblock In \textit{Proceedings of the 2nd Workshop on NLP for Conversational AI}, pages 38--45.

\bibitem[Google(2024)]{google2024gemma}
Google DeepMind (2024).
\newblock Gemma: Open Models Based on Gemini Research and Technology.
\newblock Technical Report. \url{https://ai.google.dev/gemma}

\bibitem[Microsoft(2024)]{microsoft2024phi3}
Microsoft Research (2024).
\newblock Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.
\newblock arXiv preprint arXiv:2404.14219.

\bibitem[Alibaba(2024)]{alibaba2024qwen}
Alibaba DAMO Academy (2024).
\newblock Qwen Technical Report.
\newblock \url{https://github.com/QwenLM/Qwen}

\bibitem[Mistral(2024)]{mistral2024ministral}
Mistral AI (2024).
\newblock Ministral 3B: Efficient Edge Deployment.
\newblock Technical Documentation.

\bibitem[Wang et al.(2024)]{wang2024slmedge}
Wang, J., Li, X., et Chen, Y. (2024).
\newblock Characterizing and Understanding the Performance of Small Language Models on Edge Devices.
\newblock In \textit{Proceedings of IEEE International Conference on Edge Computing}, pages 112--121.

\bibitem[Prieto et Abad(2025)]{prieto2025edge}
Prieto, P. et Abad, P. (2025).
\newblock Edge Deployment of Small Language Models: A Comprehensive Comparison of CPU, GPU and NPU Backends.
\newblock \textit{Semantic Scholar}.

\bibitem[Apple(2024)]{apple2024mlx}
Apple Machine Learning Research (2024).
\newblock MLX: An Array Framework for Apple Silicon.
\newblock \url{https://ml-explore.github.io/mlx/}

\bibitem[Schall(2025)]{schall2025mlx}
Schall, M. (2025).
\newblock Apple MLX vs. NVIDIA: How Local AI Inference Works on the Mac.
\newblock Technical Analysis, \url{https://medium.com/@mschall/mlx-vs-nvidia}. Consulté le 2025-01.

\bibitem[llama.cpp(2024)]{llamacpp2024}
Gerganov, G. et contributeurs (2024).
\newblock llama.cpp: Inference of LLaMA models in pure C/C++.
\newblock GitHub Repository. \url{https://github.com/ggerganov/llama.cpp}

\bibitem[Hassan et al.(2024)]{hassan2024banking}
Hassan, A. et al. (2024).
\newblock Intent Classification for Bank Chatbots through LLM Fine-Tuning.
\newblock arXiv preprint arXiv:2410.04925.

\bibitem[Malo et al.(2014)]{malo2014fpb}
Malo, P., Sinha, A., Korhonen, P., Wallenius, J., et Takala, P. (2014).
\newblock Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts.
\newblock \textit{Journal of the Association for Information Science and Technology}, 65(4):782--796.

\bibitem[NIST(2023)]{nist2023rmf}
National Institute of Standards and Technology (2023).
\newblock Artificial Intelligence Risk Management Framework (AI RMF 1.0).
\newblock NIST.AI.100-1. \url{https://www.nist.gov/itl/ai-risk-management-framework}

\bibitem[NIST(2024)]{nist2024genai}
National Institute of Standards and Technology (2024).
\newblock AI RMF Generative AI Profile (NIST AI 600-1).
\newblock \url{https://www.nist.gov/publications}

\bibitem[OWASP(2025)]{owasp2025top10}
OWASP Foundation (2025).
\newblock OWASP Top 10 for Large Language Model Applications.
\newblock \url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}

\bibitem[Fischer(2024)]{fischer2024maturity}
Fischer, A. (2024).
\newblock Evolving AI Risk Management: A Maturity Model based on the NIST AI Risk Management Framework.
\newblock arXiv preprint arXiv:2401.15229.

\bibitem[Zhang et al.(2025)]{zhang2025jailbreak}
Zhang, W., Liu, H., et Wang, S. (2025).
\newblock Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models.
\newblock In \textit{Findings of the Association for Computational Linguistics: ACL 2025}.

\bibitem[LiteLMGuard(2025)]{litelmguard2025}
Anonyme (2025).
\newblock LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models.
\newblock arXiv preprint arXiv:2505.05619.

\bibitem[IBM(2024)]{ibm2024genai}
IBM (2024).
\newblock Integrating Gen AI into the Financial Regulatory Framework.
\newblock IBM Think Insights.

\bibitem[Fed-RD(2024)]{fedrd2024}
Anonyme (2024).
\newblock Fed-RD: Privacy-Preserving Federated Learning for Financial Crime Detection.
\newblock arXiv preprint arXiv:2408.01609.

\bibitem[Premai(2025)]{premai2025slm}
Premai.io (2025).
\newblock Small Language Models (SLMs) for Efficient Edge Deployment.
\newblock Industry White Paper, \url{https://docs.premai.io/}. Consulté le 2025-01.

\end{thebibliography}

\newpage
% ============================================
% ANNEXES
% ============================================
\appendix

\section{Détails des Prompts d'Évaluation}
\label{app:prompts}

\subsection{Classification d'Intents Banking77}

\begin{verbatim}
System: You are a banking intent classifier. Classify the customer 
query into exactly one of the 77 banking intent categories.

User: Classify the following banking query. Respond with only the 
intent category name, nothing else.

Query: "{query}"

Intent:
\end{verbatim}

\subsection{Sentiment Financial PhraseBank}

\begin{verbatim}
System: You are a financial sentiment analyzer.

User: Analyze the sentiment of the following financial statement.
Respond with exactly one word: positive, neutral, or negative.

Statement: "{statement}"

Sentiment:
\end{verbatim}

\section{Liste des 77 Intents Banking77}
\label{app:intents}

Les 77 catégories d'intents du dataset Banking77 couvrent les domaines suivants :

\begin{itemize}
    \item \textbf{Cartes} : activate\_my\_card, card\_about\_to\_expire, card\_arrival, card\_delivery\_estimate, card\_linking, card\_not\_working, card\_payment\_fee\_charged, card\_payment\_not\_recognised, card\_payment\_wrong\_exchange\_rate, card\_swallowed, compromised\_card, declined\_card\_payment, declined\_cash\_withdrawal, getting\_spare\_card, getting\_virtual\_card, lost\_or\_stolen\_card, lost\_or\_stolen\_phone, order\_physical\_card, pin\_blocked, receiving\_money, request\_refund, reverted\_card\_payment, topping\_up\_by\_card, unable\_topping\_up\_by\_card, why\_verify\_identity, wrong\_amount\_of\_cash\_received
    
    \item \textbf{Transactions} : balance\_not\_updated\_after\_bank\_transfer, balance\_not\_updated\_after\_cheque\_or\_cash\_deposit, beneficiary\_not\_allowed, cancel\_transfer, direct\_debit\_payment\_not\_recognised, failed\_transfer, pending\_card\_payment, pending\_cash\_withdrawal, pending\_top\_up, pending\_transfer, Refund\_not\_showing\_up, top\_up\_by\_bank\_transfer\_charge, top\_up\_by\_card\_charge, top\_up\_by\_cash\_or\_cheque, top\_up\_failed, top\_up\_limits, top\_up\_reverted, transaction\_charged\_twice, transfer\_fee\_charged, transfer\_into\_account, transfer\_not\_received\_by\_recipient, transfer\_timing, unable\_to\_verify\_identity, verify\_my\_identity, verify\_source\_of\_funds, verify\_top\_up, wrong\_exchange\_rate\_for\_cash\_withdrawal
    
    \item \textbf{Compte} : age\_limit, apple\_pay\_or\_google\_pay, atm\_support, automatic\_top\_up, change\_pin, contactless\_not\_working, country\_support, disposable\_card\_limits, edit\_personal\_details, exchange\_charge, exchange\_rate, exchange\_via\_app, extra\_charge\_on\_statement, fiat\_currency\_support, get\_disposable\_virtual\_card, get\_physical\_card, passcode\_forgotten, supported\_cards\_and\_currencies, terminate\_account, top\_up\_by\_card\_charge, top\_up\_by\_cash\_or\_cheque, virtual\_card\_not\_working, visa\_or\_mastercard, why\_verify\_identity
    
    \item \textbf{Général} : cash\_withdrawal\_charge, cash\_withdrawal\_not\_recognised, exchange\_charge, fiat\_currency\_support, supported\_cards\_and\_currencies
\end{itemize}

\section{Configuration YAML Complète}
\label{app:config}

\begin{verbatim}
# models.yaml
models:
  - id: gemma-3n-e4b-mlx
    name: Gemma 3n E4B
    format: mlx
    quantization: 4bit
    parameters: 4B
    context_length: 32768
    
  - id: gemma-3n-e4b-gguf
    name: Gemma 3n E4B
    format: gguf
    quantization: Q4_K_M
    parameters: 4B
    context_length: 32768
    
  - id: qwen3-vl-4b-mlx
    name: Qwen3-VL 4B
    format: mlx
    quantization: 4bit
    parameters: 4B
    context_length: 262144
    
  - id: qwen3-vl-4b-gguf
    name: Qwen3-VL 4B
    format: gguf
    quantization: Q4_K_M
    parameters: 4B
    context_length: 262144
    
  - id: ministral-3-3b-gguf
    name: Ministral 3 3B
    format: gguf
    quantization: Q4_K_M
    parameters: 3B
    context_length: 32768

# sampling_params.yaml
sampling:
  temperature: 0.0
  top_p: 1.0
  max_tokens: 512
  seed: 42
  
# benchmark.yaml
benchmark:
  runs_per_scenario: 200
  warmup_runs: 3
  cooldown_seconds: 2
  bootstrap_iterations: 10000
  confidence_level: 0.95
\end{verbatim}

\end{document}
