# Tutoriel : Edge SLM Benchmark Framework

## Guide Pas √† Pas pour R√©aliser ton Projet de Recherche

Ce tutoriel te guide √©tape par √©tape pour ex√©cuter tous les benchmarks et produire les r√©sultats n√©cessaires √† ton article de recherche.

---

## Table des Mati√®res

1. [Phase 1 : Pr√©paration de l'environnement](#phase-1--pr√©paration-de-lenvironnement)
2. [Phase 2 : Configuration de LM Studio](#phase-2--configuration-de-lm-studio)
3. [Phase 3 : Benchmarks de Performance](#phase-3--benchmarks-de-performance)
4. [Phase 4 : Benchmarks de Capacit√©s Banking](#phase-4--benchmarks-de-capacit√©s-banking)
5. [Phase 5 : Analyse de Conformit√©](#phase-5--analyse-de-conformit√©)
6. [Phase 6 : G√©n√©ration des Rapports](#phase-6--g√©n√©ration-des-rapports)
7. [Phase 7 : Interpr√©tation des R√©sultats](#phase-7--interpr√©tation-des-r√©sultats)
8. [Troubleshooting](#troubleshooting)

---

## Phase 1 : Pr√©paration de l'environnement

### √âtape 1.1 : V√©rifier les pr√©requis

Avant de commencer, assure-toi d'avoir :

```bash
# V√©rifier la version de Python (3.10+ requis)
python3 --version

# V√©rifier que tu es sur Apple Silicon
uname -m
# Doit afficher : arm64
```

### √âtape 1.2 : Cr√©er l'environnement virtuel

```bash
# Naviguer vers le dossier du projet
cd /Users/lotfi/Documents/Projet/edge_benchmark

# Cr√©er l'environnement virtuel
python3 -m venv venv

# Activer l'environnement
source venv/bin/activate

# V√©rifier que tu es dans le bon environnement
which python
# Doit afficher : /Users/lotfi/Documents/Projet/edge_benchmark/venv/bin/python
```

### √âtape 1.3 : Installer les d√©pendances

```bash
# Installer toutes les d√©pendances
pip install -r requirements.txt

# V√©rifier l'installation
python -c "from src import LMStudioClient; print('‚úÖ Installation OK')"
```

**Temps estim√© :** 5-10 minutes

---

## Phase 2 : Configuration de LM Studio

### √âtape 2.1 : T√©l√©charger les mod√®les

1. Ouvrir **LM Studio**
2. Aller dans **Discover** (recherche de mod√®les)
3. T√©l√©charger les mod√®les suivants :

#### Mod√®les GGUF (format universel)

| Mod√®le | Recherche | Format √† choisir |
|--------|-----------|------------------|
| Gemma 3n E4B | `gemma 3n e4b` | **Q4_K_M (GGUF)** |
| Qwen3-VL 4B | `qwen3 vl 4b` | **Q4_K_M (GGUF)** |
| Ministral 3 3B | `ministral 3b` | **Q4_K_M (GGUF)** |

#### Mod√®les MLX (optimis√© Apple Silicon)

| Mod√®le | Recherche | Format √† choisir |
|--------|-----------|------------------|
| Gemma 3n E4B | `gemma 3n e4b mlx` | **4bit (MLX)** |
| Qwen3-VL 4B | `qwen3 vl 4b mlx` | **4bit (MLX)** |

> **‚ö†Ô∏è IMPORTANT : Formats MLX vs GGUF**
> 
> LM Studio utilise le **m√™me ID** pour un mod√®le, peu importe le format (MLX ou GGUF).
> Par exemple, `google/gemma-3n-e4b` peut √™tre la version MLX ou GGUF.
> 
> **Tu dois charger manuellement le bon format** dans LM Studio avant chaque benchmark.
> Le script t'indiquera quel format est attendu avant de lancer les tests.

### √âtape 2.2 : D√©marrer le serveur LM Studio

1. Aller dans l'onglet **Developer** (ou **Local Server**)
2. S√©lectionner un mod√®le (commence par Gemma)
3. Cliquer sur **Start Server**
4. V√©rifier que le serveur est d√©marr√© sur `http://localhost:1234`

### √âtape 2.3 : Tester la connexion

```bash
# Dans ton terminal (avec venv activ√©)
python -c "
from src.lmstudio_client import LMStudioClient
client = LMStudioClient()
health = client.health_check()
print(f'Status: {health}')
models = client.list_models()
print(f'Mod√®les disponibles: {len(models)}')
for m in models:
    print(f'  - {m}')
"
```

**Tu dois voir :** Le statut "healthy" et la liste des mod√®les.

**Temps estim√© :** 10-30 minutes (selon ta connexion pour t√©l√©charger les mod√®les)

---

## Phase 3 : Benchmarks de Performance

### Objectif

Mesurer TTFT (Time To First Token), tokens/s, et consommation RAM pour chaque mod√®le.

### √âtape 3.1 : Test rapide (v√©rifier que tout fonctionne)

Avant de lancer un benchmark complet, fais un test rapide :

**1. Dans LM Studio :** Charge le mod√®le `Gemma 3n E4B` en format **GGUF Q4_K_M**

**2. Lance le test :**

```bash
# Test rapide : 5 runs seulement
python scripts/run_performance.py \
    --models gemma_3n_e4b_gguf \
    --scenarios interactive_assistant \
    --runs 5 \
    --warmup 1
```

**Ce que tu dois voir :**
```
============================================================
EDGE SLM PERFORMANCE BENCHMARK
============================================================
Models (1):
  - Gemma 3n E4B (GGUF Q4_K_M) [GGUF]
Scenarios: ['interactive_assistant']
...

‚ö†Ô∏è  IMPORTANT: Assurez-vous que le mod√®le suivant est charg√© dans LM Studio:
    ID: google/gemma-3n-e4b
    Format attendu: GGUF Q4_K_M

[Check] Model is responding ‚úì
...

RESULTS SUMMARY
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TTFT:           XXX.X ms [95% CI: XXX.X, XXX.X]
Output t/s:     XX.X [95% CI: XX.X, XX.X]
Peak RAM:       XXXX.X MB
Success rate:   100.0%
Duration:       XX.Xs
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[Saved] Results saved to .../perf_google_gemma-3n-e4b_GGUF_Q4_K_M_interactive_assistant_XXXXXX.jsonl
```

> **Note :** Le nom du fichier de r√©sultats inclut maintenant le format (GGUF/MLX) et la quantization pour distinguer les tests.

### √âtape 3.2 : Benchmark complet - Un mod√®le √† la fois

**Important :** Lance les mod√®les un par un pour √©viter les probl√®mes. LM Studio ne peut charger qu'un mod√®le √† la fois.

---

#### PARTIE A : Mod√®les GGUF

##### Mod√®le 1 : Gemma 3n E4B (GGUF)

1. Dans LM Studio : Charger `Gemma 3n E4B` au format **GGUF Q4_K_M**
2. V√©rifier que le serveur est d√©marr√©
3. Lancer le benchmark :

```bash
python scripts/run_performance.py \
    --models gemma_3n_e4b_gguf \
    --scenarios all \
    --runs 20 \
    --seed 42
```

**Fichiers g√©n√©r√©s :** `perf_google_gemma-3n-e4b_GGUF_Q4_K_M_*.jsonl`

**Temps estim√© :** 30-45 minutes

##### Mod√®le 2 : Qwen3-VL 4B (GGUF)

1. Dans LM Studio : **Stopper** le serveur, charger `Qwen3-VL 4B` au format **GGUF Q4_K_M**, **red√©marrer** le serveur
2. Lancer le benchmark :

```bash
python scripts/run_performance.py \
    --models qwen3_vl_4b_gguf \
    --scenarios all \
    --runs 20 \
    --seed 42
```

**Fichiers g√©n√©r√©s :** `perf_qwen_qwen3-vl-4b_GGUF_Q4_K_M_*.jsonl`

##### Mod√®le 3 : Ministral 3 3B (GGUF)

1. Dans LM Studio : Changer pour `Ministral 3 3B` au format **GGUF Q4_K_M**
2. Lancer le benchmark :

```bash
python scripts/run_performance.py \
    --models ministral_3_3b_gguf \
    --scenarios all \
    --runs 20 \
    --seed 42
```

**Fichiers g√©n√©r√©s :** `perf_mistralai_ministral-3-3b_GGUF_Q4_K_M_*.jsonl`

---

#### PARTIE B : Mod√®les MLX (pour comparaison GGUF vs MLX)

> **Objectif :** Comparer les performances GGUF vs MLX sur Apple Silicon

##### Mod√®le 4 : Gemma 3n E4B (MLX)

1. Dans LM Studio : Charger `Gemma 3n E4B` au format **MLX 4bit**
2. Lancer le benchmark :

```bash
python scripts/run_performance.py \
    --models gemma_3n_e4b_mlx \
    --scenarios all \
    --runs 20 \
    --seed 42
```

**Fichiers g√©n√©r√©s :** `perf_google_gemma-3n-e4b_MLX_4bit_*.jsonl`

##### Mod√®le 5 : Qwen3-VL 4B (MLX)

1. Dans LM Studio : Charger `Qwen3-VL 4B` au format **MLX 4bit**
2. Lancer le benchmark :

```bash
python scripts/run_performance.py \
    --models qwen3_vl_4b_mlx \
    --scenarios all \
    --runs 20 \
    --seed 42
```

**Fichiers g√©n√©r√©s :** `perf_qwen_qwen3-vl-4b_MLX_4bit_*.jsonl`

---

> **R√©capitulatif des 5 benchmarks √† lancer :**
> 
> | # | Mod√®le | Format | Cl√© config |
> |---|--------|--------|------------|
> | 1 | Gemma 3n E4B | GGUF Q4_K_M | `gemma_3n_e4b_gguf` |
> | 2 | Qwen3-VL 4B | GGUF Q4_K_M | `qwen3_vl_4b_gguf` |
> | 3 | Ministral 3 3B | GGUF Q4_K_M | `ministral_3_3b_gguf` |
> | 4 | Gemma 3n E4B | MLX 4bit | `gemma_3n_e4b_mlx` |
> | 5 | Qwen3-VL 4B | MLX 4bit | `qwen3_vl_4b_mlx` |

### √âtape 3.3 : V√©rifier les r√©sultats

```bash
# Lister les fichiers de r√©sultats
ls -la results/perf_*.jsonl

# Afficher un aper√ßu
head -n 5 results/perf_*.jsonl
```

### En cas de crash

Si le benchmark s'interrompt :

```bash
# Reprendre l√† o√π tu t'es arr√™t√©
python scripts/run_performance.py --resume
```

---

## Phase 4 : Benchmarks de Capacit√©s Banking

### Objectif

√âvaluer la pr√©cision des mod√®les sur des t√¢ches bancaires r√©elles.

### √âtape 4.1 : Banking77 (Classification d'intents)

C'est le **benchmark principal** pour ton article.

```bash
# Pour chaque mod√®le, faire :

# 1. Gemma (charger dans LM Studio d'abord)
python scripts/run_capability.py \
    --task banking77 \
    --model google/gemma-3n-e4b

# 2. Qwen (changer le mod√®le dans LM Studio)
python scripts/run_capability.py \
    --task banking77 \
    --model qwen/qwen3-vl-4b

# 3. Ministral (changer le mod√®le dans LM Studio)
python scripts/run_capability.py \
    --task banking77 \
    --model mistralai/ministral-3-3b
```

**Temps estim√© :** 20-40 minutes par mod√®le (3000+ √©chantillons)

### √âtape 4.2 : Financial PhraseBank (Sentiment)

```bash
# Pour chaque mod√®le :
python scripts/run_capability.py \
    --task financial_phrasebank \
    --model google/gemma-3n-e4b \
    --sample-size 1000
```

**Temps estim√© :** 15-25 minutes par mod√®le

### √âtape 4.3 : Test de codage (optionnel mais recommand√©)

```bash
python scripts/run_capability.py \
    --task coding \
    --model google/gemma-3n-e4b \
    --sample-size 30
```

**Temps estim√© :** 10-15 minutes par mod√®le

### √âtape 4.4 : Sc√©narios r√©alistes banking

```bash
python scripts/run_capability.py \
    --task realistic \
    --model google/gemma-3n-e4b
```

### V√©rifier les r√©sultats

```bash
# Lister les r√©sultats
ls -la results/eval_*.json results/realistic_*.json

# Voir un r√©sum√©
cat results/eval_banking77_*.json | python -m json.tool | head -30
```

---

## Phase 5 : Analyse de Conformit√©

### Objectif

G√©n√©rer les rapports d'analyse de risques et d'audit des licences.

### √âtape 5.1 : G√©n√©rer les rapports de conformit√©

```bash
# Cette commande g√©n√®re automatiquement :
# - risk_analysis_*.json (analyse NIST/OWASP)
# - license_audit_*.json (audit des licences)

python scripts/generate_report.py --compliance --output report/
```

### √âtape 5.2 : Examiner les r√©sultats

```bash
# Voir le r√©sum√© des risques
cat report/compliance/risk_analysis_*.json | python -m json.tool | head -50

# Voir l'audit des licences
cat report/compliance/license_audit_*.json | python -m json.tool
```

---

## Phase 6 : G√©n√©ration des Rapports

### √âtape 6.1 : G√©n√©rer le rapport complet

```bash
python scripts/generate_report.py \
    --input results/ \
    --output report/ \
    --format all \
    --compliance
```

### √âtape 6.2 : Fichiers g√©n√©r√©s

Apr√®s cette commande, tu auras dans `report/` :

| Fichier | Description |
|---------|-------------|
| `benchmark_report_*.md` | Rapport Markdown complet |
| `benchmark_report_*.html` | Rapport HTML (visualisation navigateur) |
| `performance_results.csv` | Tableau des performances |
| `capability_results.csv` | Tableau des capacit√©s |
| `compliance/risk_analysis_*.json` | Analyse de risques |
| `compliance/license_audit_*.json` | Audit des licences |

### √âtape 6.3 : Visualiser le rapport HTML

```bash
# Ouvrir le rapport dans le navigateur
open report/benchmark_report_*.html
```

---

## Phase 7 : Interpr√©tation des R√©sultats

### 7.1 M√©triques de Performance

| M√©trique | Bon | Moyen | Mauvais | Interpr√©tation |
|----------|-----|-------|---------|----------------|
| TTFT | < 300ms | 300-600ms | > 600ms | Temps avant premi√®re r√©ponse |
| tokens/s | > 40 | 20-40 | < 20 | Vitesse de g√©n√©ration |
| Peak RAM | < 8GB | 8-12GB | > 12GB | Sur laptop 16GB |

### 7.2 M√©triques de Capacit√©s

| M√©trique | Bon | Acceptable | Insuffisant |
|----------|-----|------------|-------------|
| Banking77 Accuracy | > 70% | 50-70% | < 50% |
| Macro-F1 | > 60% | 40-60% | < 40% |
| Financial Sentiment Acc | > 75% | 60-75% | < 60% |

### 7.3 Ce qui compte pour ton article

1. **Comparaison GGUF vs MLX** : Montre les diff√©rences de performance sur Apple Silicon
2. **Trade-off Performance vs Accuracy** : Le mod√®le le plus rapide est-il le plus pr√©cis ?
3. **Viabilit√© banking** : Les mod√®les atteignent-ils un niveau acceptable pour le contexte bancaire ?
4. **Conformit√©** : Quels risques r√©siduels ? Quelles licences sont compatibles ?

---

## Troubleshooting

### Probl√®me : "LM Studio server not healthy"

**Solution :**
1. V√©rifier que LM Studio est d√©marr√©
2. V√©rifier que le serveur local est activ√©
3. V√©rifier le port : `curl http://localhost:1234/v1/models`

### Probl√®me : "Out of memory"

**Solution :**
1. Fermer les autres applications
2. Utiliser un mod√®le plus petit (Q4 au lieu de Q8)
3. R√©duire `--runs` √† 10

### Probl√®me : "Model not found" ou toutes les m√©triques √† 0

**Solution :**
1. V√©rifier que le **bon format** est charg√© (MLX vs GGUF)
2. Le script affiche maintenant le format attendu :
   ```
   ‚ö†Ô∏è  IMPORTANT: Assurez-vous que le mod√®le suivant est charg√© dans LM Studio:
       ID: google/gemma-3n-e4b
       Format attendu: GGUF Q4_K_M
   ```
3. V√©rifier l'ID exact du mod√®le : `curl http://localhost:1234/v1/models`
4. S'assurer que le mod√®le est bien **charg√©** (pas juste t√©l√©charg√©)

### Probl√®me : Confusion MLX vs GGUF

**Solution :**
- Les fichiers de r√©sultats incluent maintenant le format dans leur nom :
  - `perf_..._GGUF_Q4_K_M_...jsonl` ‚Üí Version GGUF
  - `perf_..._MLX_4bit_...jsonl` ‚Üí Version MLX
- Les r√©sultats JSON contiennent aussi `model_format` et `model_quantization`

### Probl√®me : Benchmark interrompu

**Solution :**
```bash
# Reprendre l√† o√π tu t'es arr√™t√©
python scripts/run_performance.py --resume
python scripts/run_capability.py --resume
```

### Probl√®me : R√©sultats incoh√©rents

**Solution :**
1. V√©rifier que le bon mod√®le est charg√© dans LM Studio
2. S'assurer que la machine est branch√©e
3. D√©sactiver le mode √©conomie d'√©nergie
4. Fermer les applications lourdes (navigateur, etc.)

---

## Checklist Finale

### Avant de r√©diger ton article, v√©rifie :

- [ ] Benchmark performance ex√©cut√© pour les 3 mod√®les GGUF
- [ ] Benchmark performance ex√©cut√© pour les 2 mod√®les MLX (comparaison)
- [ ] Banking77 ex√©cut√© pour tous les mod√®les
- [ ] Financial PhraseBank ex√©cut√© pour tous les mod√®les
- [ ] Rapports de conformit√© g√©n√©r√©s
- [ ] CSV export√©s pour les tableaux de l'article
- [ ] Screenshots/visualisations pr√™ts

### V√©rifier que les fichiers de r√©sultats sont complets :

```bash
# Lister tous les r√©sultats avec le format visible
ls results/perf_*_GGUF_*.jsonl  # R√©sultats GGUF
ls results/perf_*_MLX_*.jsonl   # R√©sultats MLX
```

### Fichiers √† inclure dans l'article

1. **Tableau Performance** : `report/performance_results.csv`
2. **Tableau Capacit√©s** : `report/capability_results.csv`
3. **Analyse de risques** : `report/compliance/risk_analysis_*.json`
4. **Audit licences** : `report/compliance/license_audit_*.json`

### Pour la reproductibilit√©

Inclure dans le paper :
- Le **config hash** (trouv√© dans `results/experiment_config_*.json`)
- Les **seeds** utilis√©s (42 par d√©faut)
- La **version LM Studio**
- Les **sp√©cifications hardware** (captur√©es automatiquement)

---

## Timeline Recommand√©e

| Phase | Dur√©e estim√©e |
|-------|---------------|
| Phase 1 : Setup | 15-30 min |
| Phase 2 : LM Studio | 30-60 min (t√©l√©chargement mod√®les) |
| Phase 3 : Performance GGUF | 2-3 heures (3 mod√®les √ó 3 sc√©narios) |
| Phase 3 : Performance MLX | 1-2 heures (2 mod√®les √ó 3 sc√©narios) |
| Phase 4 : Capacit√©s | 3-4 heures (3 mod√®les √ó 2-3 t√¢ches) |
| Phase 5 : Conformit√© | 10 min |
| Phase 6 : Rapports | 5 min |
| **Total** | **8-10 heures** |

**Note :** Tu peux √©taler sur plusieurs jours gr√¢ce au syst√®me de checkpoint !

### Workflow recommand√© pour changer de mod√®le/format

1. **Arr√™ter** le serveur LM Studio (bouton Stop)
2. **S√©lectionner** le nouveau mod√®le/format dans l'onglet "My Models"
3. **D√©marrer** le serveur (bouton Start)
4. **Attendre** que le mod√®le soit compl√®tement charg√© (barre de progression √† 100%)
5. **Lancer** le benchmark correspondant

---

## Aide Suppl√©mentaire

Si tu as des questions :
1. V√©rifie les logs dans le terminal
2. Consulte les fichiers de r√©sultats dans `results/`
3. Regarde les checkpoints : `cat results/checkpoint_*.json`

Bon benchmark ! üöÄ

