# Edge SLM Benchmark - Evaluation Tasks Configuration
# Definition of capability evaluation tasks

# === MINI-BENCHMARK LM-EVALUATION-HARNESS ===
# Objective: Quick validation, not exhaustive
# Estimated duration: ~30min-1h per model

harness_tasks:
  # MMLU - Multi-domain knowledge (subset)
  mmlu_mini:
    task_name: "mmlu"
    description: "Massive Multitask Language Understanding (subset)"
    framework: "lm-evaluation-harness"
    
    config:
      num_fewshot: 5
      limit: 200              # Subset of 200 questions
      batch_size: 4
      
    subjects_subset:          # Banking-relevant domains
      - "professional_accounting"
      - "business_ethics"
      - "management"
      - "marketing"
      - "professional_law"
      
    metrics:
      - accuracy
      - accuracy_stderr
      
    estimated_time_minutes: 20

  # GSM8K - Mathematical reasoning (subset)
  gsm8k_mini:
    task_name: "gsm8k"
    description: "Grade School Math 8K (subset)"
    framework: "lm-evaluation-harness"
    
    config:
      num_fewshot: 5
      limit: 100              # Subset of 100 problems
      batch_size: 4
      
    metrics:
      - exact_match
      - accuracy
      
    estimated_time_minutes: 15

# === BANKING TASKS ===
banking_tasks:
  # Banking77 - Intent classification
  banking77:
    name: "Banking77 Intent Classification"
    description: "Classification of banking queries into 77 intent categories"
    source: "PolyAI/banking77"
    framework: "lmstudio"
    
    dataset:
      huggingface_id: "PolyAI/banking77"
      split: "test"
      sample_size: null       # Use full test set (~3k examples)
      
    task_type: "classification"
    num_classes: 77
    
    prompt_template: |
      Classify the following banking customer query into one of the intent categories.
      
      Query: {text}
      
      Respond with ONLY the intent category name, nothing else.
      
    metrics:
      - accuracy
      - macro_f1
      - per_class_f1
      
    evaluation_settings:
      temperature: 0
      max_tokens: 50
      
  # Financial PhraseBank - Sentiment
  # Note: The original takala/financial_phrasebank dataset uses a legacy script
  # that is no longer supported by the datasets library. We use ChanceFocus/flare-fpb
  # which contains the same data in modern Parquet format.
  financial_phrasebank:
    name: "Financial PhraseBank Sentiment"
    description: "Sentiment analysis on financial news (via flare-fpb)"
    source: "ChanceFocus/flare-fpb"
    framework: "lmstudio"
    
    dataset:
      huggingface_id: "ChanceFocus/flare-fpb"
      split: "test"                 # Test split for evaluation
      sample_size: 970              # Test split size
      
    task_type: "classification"
    num_classes: 3
    class_labels:
      - "positive"
      - "negative"
      - "neutral"
      
    prompt_template: |
      Analyze the sentiment of the following financial news sentence.
      
      Sentence: {sentence}
      
      Respond with exactly one word: positive, negative, or neutral.
      
    metrics:
      - accuracy
      - macro_f1
      - confusion_matrix
      
    evaluation_settings:
      temperature: 0
      max_tokens: 10

  # FinTextQA - Finance QA (optional)
  fintextqa:
    name: "FinTextQA"
    description: "Question-Answering on financial documents"
    source: "fintextqa"
    framework: "lmstudio"
    enabled: false            # Optional, disabled by default
    
    dataset:
      source: "manual"        # Requires manual download
      sample_size: 100
      
    task_type: "qa"
    
    prompt_template: |
      Based on the following financial document, answer the question.
      
      Document: {context}
      
      Question: {question}
      
      Answer:
      
    metrics:
      - exact_match
      - f1_score
      - rouge_l

# === CODING TEST (MINI) ===
coding_tasks:
  humaneval_mini:
    name: "HumanEval Mini"
    description: "Python code generation evaluation (subset)"
    source: "openai/humaneval"
    framework: "lmstudio"
    
    dataset:
      huggingface_id: "openai/openai_humaneval"
      split: "test"
      sample_size: 30         # Subset of 30 problems
      
    task_type: "code_generation"
    language: "python"
    
    prompt_template: |
      Complete the following Python function:
      
      {prompt}
      
    metrics:
      - pass_at_1
      - syntax_valid_rate
      
    evaluation_settings:
      temperature: 0
      max_tokens: 256
      stop_sequences:
        - "\ndef "
        - "\nclass "
        - "\n#"

# === A/B SETTINGS CONFIGURATION ===
evaluation_settings:
  setting_a_realistic:
    name: "Realistic (Native Templates)"
    description: "Uses LM Studio's native chat templates"
    use_native_templates: true
    apply_chat_template: true
    
  setting_b_controlled:
    name: "Controlled (Unified Prompts)"
    description: "Unified prompt without specific template"
    use_native_templates: false
    apply_chat_template: false
    raw_prompt_format: |
      {system_prompt}
      
      {user_prompt}

# Recommended execution order
execution_order:
  priority_1_banking:
    - banking77
    - financial_phrasebank
  priority_2_coding:
    - humaneval_mini
  priority_3_harness:
    - mmlu_mini
    - gsm8k_mini
  optional:
    - fintextqa


