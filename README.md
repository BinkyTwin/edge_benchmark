# Edge SLM Benchmark Framework

**Local Generative AI on Enterprise Edge Devices in Regulated Banking**

Framework de benchmark pour Ã©valuer des Small Language Models (SLMs) sur Apple Silicon dans un contexte bancaire rÃ©glementÃ©.

---

## Table des MatiÃ¨res

1. [ProblÃ©matique](#problÃ©matique)
2. [ModÃ¨les Ã‰valuÃ©s](#modÃ¨les-Ã©valuÃ©s)
3. [Installation](#installation)
4. [Guide Rapide](#guide-rapide)
5. [Utilisation DÃ©taillÃ©e](#utilisation-dÃ©taillÃ©e)
6. [SystÃ¨me de Checkpoint](#systÃ¨me-de-checkpoint)
7. [Benchmarks Disponibles](#benchmarks-disponibles)
8. [ReproductibilitÃ©](#reproductibilitÃ©)
9. [Analyse Statistique](#analyse-statistique)
10. [ConformitÃ©](#conformitÃ©)
11. [Structure du Projet](#structure-du-projet)

---

## ProblÃ©matique

> **Measuring SLM Utility Under Latency/Privacy Constraints and Assessing Residual Compliance Risk on Consumer-Grade Macs**

Ce framework permet de rÃ©pondre Ã  la question : **"Les SLMs peuvent-ils Ãªtre dÃ©ployÃ©s efficacement sur des laptops Apple Silicon dans un contexte bancaire rÃ©glementÃ© ?"**

Les axes d'Ã©valuation :
- **Performance** : Latence (TTFT), dÃ©bit (tokens/s), consommation mÃ©moire
- **CapacitÃ©s** : Classification bancaire, analyse de sentiment, gÃ©nÃ©ration de code
- **ConformitÃ©** : Analyse de risques NIST/OWASP, audit des licences

---

## ModÃ¨les Ã‰valuÃ©s

| Model | Publisher | Format | Quantization | Context | Type |
|-------|-----------|--------|--------------|---------|------|
| Gemma 3n E4B | Google | MLX / GGUF | 4bit / Q4_K_M | 32K | VLM |
| Qwen3-VL 4B | Alibaba | MLX / GGUF | 4bit / Q4_K_M | 262K | VLM |
| Ministral 3 3B | Mistral AI | GGUF | Q4_K_M | 262K | VLM |

---

## Installation

### PrÃ©requis

- **macOS** avec Apple Silicon (M1/M2/M3/M4)
- **Python 3.10+**
- **LM Studio** installÃ© avec les modÃ¨les tÃ©lÃ©chargÃ©s
- **16 GB RAM minimum** (recommandÃ©)

### Ã‰tapes d'installation

```bash
# 1. Cloner le repository (ou naviguer vers le dossier)
cd /path/to/edge_benchmark

# 2. CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. VÃ©rifier l'installation
python -c "from src import LMStudioClient; print('OK')"
```

### Configuration de LM Studio

1. Ouvrir **LM Studio**
2. TÃ©lÃ©charger les modÃ¨les (Gemma, Qwen, Ministral)
3. DÃ©marrer le serveur local : **Developer** â†’ **Start Server**
4. VÃ©rifier que le serveur est accessible sur `http://localhost:1234`

---

## Guide Rapide

### Lancer un benchmark en 3 commandes

```bash
# Activer l'environnement
source venv/bin/activate

# Lancer un benchmark Banking77 sur Gemma
python scripts/run_capability.py --task banking77 --model google/gemma-3n-e4b

# GÃ©nÃ©rer le rapport
python scripts/generate_report.py --input results/ --output report/
```

---

## Utilisation DÃ©taillÃ©e

### 1. Benchmark de Performance

Ã‰value la latence, le dÃ©bit et la consommation mÃ©moire.

```bash
# Tous les modÃ¨les, tous les scÃ©narios
python scripts/run_performance.py --models all --scenarios all

# Un seul modÃ¨le, un seul scÃ©nario
python scripts/run_performance.py --models gemma_3n_e4b_gguf --scenarios interactive_assistant

# Comparaison de tous les modÃ¨les sur un scÃ©nario
python scripts/run_performance.py --compare --scenarios interactive_assistant

# Personnaliser le nombre de runs
python scripts/run_performance.py --models all --runs 10 --warmup 2
```

**Options disponibles :**

| Option | Description | DÃ©faut |
|--------|-------------|--------|
| `--models` | `all`, `gguf`, `mlx`, ou ID spÃ©cifique | `all` |
| `--scenarios` | `all`, `interactive_assistant`, `long_form_summarization`, `structured_json_output` | `all` |
| `--runs` | Nombre de runs par scÃ©nario | 20 |
| `--warmup` | RequÃªtes de warm-up | 3 |
| `--cooldown` | Pause entre runs (secondes) | 2.0 |
| `--seed` | Seed pour reproductibilitÃ© | 42 |
| `--resume` | Reprendre depuis le checkpoint | - |

### 2. Benchmark de CapacitÃ©s (Banking)

Ã‰value les capacitÃ©s des modÃ¨les sur des tÃ¢ches bancaires.

```bash
# Banking77 - Classification d'intents (77 classes)
python scripts/run_capability.py --task banking77 --model google/gemma-3n-e4b

# Financial PhraseBank - Analyse de sentiment
python scripts/run_capability.py --task financial_phrasebank --model google/gemma-3n-e4b

# Les deux tÃ¢ches banking
python scripts/run_capability.py --task banking_all --model google/gemma-3n-e4b

# ScÃ©narios rÃ©alistes (FAQ, extraction, rÃ©sumÃ©)
python scripts/run_capability.py --task realistic --model google/gemma-3n-e4b

# Test de codage (HumanEval mini)
python scripts/run_capability.py --task coding --model google/gemma-3n-e4b

# Tous les modÃ¨les sur une tÃ¢che
python scripts/run_capability.py --task banking77 --all-models

# Limiter le nombre d'Ã©chantillons (pour tests rapides)
python scripts/run_capability.py --task banking77 --model google/gemma-3n-e4b --sample-size 100
```

**TÃ¢ches disponibles :**

| TÃ¢che | Description | MÃ©triques |
|-------|-------------|-----------|
| `banking77` | Classification d'intents bancaires | Accuracy, Macro-F1 |
| `financial_phrasebank` | Sentiment sur news financiÃ¨res | Accuracy, Macro-F1 |
| `banking_all` | Les deux tÃ¢ches ci-dessus | - |
| `realistic` | FAQ, sentiment, extraction, rÃ©sumÃ© | Diverses |
| `coding` | HumanEval (30 problÃ¨mes) | Pass@1 |
| `harness` | MMLU + GSM8K via lm-eval | Accuracy |
| `all` | Toutes les tÃ¢ches | - |

### 3. Mini-benchmark Harness (MMLU/GSM8K)

Pour les benchmarks acadÃ©miques nÃ©cessitant des logprobs :

```bash
# NÃ©cessite le chemin vers le fichier GGUF
python scripts/run_capability.py --task harness --gguf-path ~/.cache/lm-studio/models/gemma-3n-e4b-q4_k_m.gguf
```

### 4. GÃ©nÃ©ration de Rapports

```bash
# Rapport Markdown + HTML + CSV
python scripts/generate_report.py --input results/ --output report/ --format all

# Avec rapports de conformitÃ©
python scripts/generate_report.py --input results/ --output report/ --compliance
```

---

## SystÃ¨me de Checkpoint

Le framework sauvegarde automatiquement l'Ã©tat aprÃ¨s chaque modÃ¨le/scÃ©nario.

### En cas de crash ou interruption

```bash
# Le script affiche :
[Interrupted] Checkpoint saved. Use --resume to continue.

# Pour reprendre exactement lÃ  oÃ¹ tu t'es arrÃªtÃ© :
python scripts/run_performance.py --resume
python scripts/run_capability.py --resume
```

### RÃ©sumÃ© du checkpoint

Ã€ la fin de chaque exÃ©cution :

```
==================================================
CHECKPOINT SUMMARY
==================================================
Experiment: exp_20241221_143052
Type: performance
Progress: 4/6 completed
Failed: 1
  - qwen/qwen3-vl-4b/summarization: Connection timeout...
Remaining: 1
  - mistralai/ministral-3-3b/interactive_assistant
==================================================

ðŸ’¡ To complete remaining tasks, run:
   python scripts/run_performance.py --resume
```

### DÃ©sactiver les checkpoints

```bash
python scripts/run_performance.py --no-checkpoint
```

---

## Benchmarks Disponibles

### Performance (3 scÃ©narios)

| ScÃ©nario | Input | Output | Focus |
|----------|-------|--------|-------|
| Interactive Assistant | 200-400 tokens | 128 tokens | TTFT, latence |
| Long-form Summarization | 2000-4000 tokens | 256-512 tokens | DÃ©bit, RAM |
| Structured JSON Output | 500-1000 tokens | JSON | Taux de validitÃ© |

### CapacitÃ©s (tÃ¢ches banking)

| Dataset | Source | Taille | TÃ¢che |
|---------|--------|--------|-------|
| Banking77 | Hugging Face | ~3K test | Intent classification |
| Financial PhraseBank | Hugging Face | ~5K | Sentiment analysis |
| HumanEval | OpenAI | 30 subset | Code generation |
| MMLU | Eleuther | 200 subset | Multi-task |
| GSM8K | Eleuther | 100 subset | Math reasoning |

---

## ReproductibilitÃ©

### Protocole de dÃ©terminisme

```python
from src.reproducibility import set_deterministic_mode

# Configure le mode dÃ©terministe global
manager = set_deterministic_mode(seed=42)
manager.capture_environment()
manager.save_experiment_config()
```

### ParamÃ¨tres fixÃ©s

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Global seed | 42 | Python random |
| NumPy seed | 42 | Sampling |
| Temperature | 0 | DÃ©terministe |
| Top-p | 1 | Pas de nucleus |

### Capture d'environnement

Chaque expÃ©rience gÃ©nÃ¨re un fichier `experiment_config_*.json` contenant :
- Hardware (chip, RAM, cores)
- Software (OS, Python, packages)
- Configs (models.yaml, scenarios.yaml)
- **Config hash SHA256** pour vÃ©rification

---

## Analyse Statistique

### Intervalles de Confiance (IC 95%)

Les mÃ©triques sont rapportÃ©es avec bootstrap (10,000 itÃ©rations) :

```
TTFT: 245.3 ms [95% CI: 238.1, 252.7]
Output t/s: 42.8 [95% CI: 40.2, 45.1]
```

### Tests de SignificativitÃ©

Pour comparer les modÃ¨les :

| Test | Conditions | Usage |
|------|------------|-------|
| Paired t-test | DonnÃ©es normales, nâ‰¥20 | MÃªmes prompts |
| Wilcoxon | Non-paramÃ©trique | Petit Ã©chantillon |
| Holm correction | Comparaisons multiples | FWER |

### Tailles d'effet

- **Cohen's d** : 0.2 petit, 0.5 moyen, 0.8 grand
- **Rank-biserial r** : mÃªmes seuils

---

## ConformitÃ©

### Framework d'analyse

- **NIST AI RMF 1.0** - Risk Management Framework
- **NIST AI 600-1** - Generative AI Profile  
- **OWASP Top 10 LLM** - Risques spÃ©cifiques LLM

### Audit des Licences

| Model | License | Commercial |
|-------|---------|------------|
| Gemma 3n E4B | Gemma Terms of Use | Restricted |
| Qwen3-VL 4B | Apache 2.0 | âœ“ Allowed |
| Ministral 3 3B | Apache 2.0 | âœ“ Allowed |

### GÃ©nÃ©rer les rapports de conformitÃ©

```bash
python scripts/generate_report.py --compliance
```

---

## Structure du Projet

```
edge_benchmark/
â”œâ”€â”€ configs/                    # Configuration YAML
â”‚   â”œâ”€â”€ models.yaml            # DÃ©finition des modÃ¨les
â”‚   â”œâ”€â”€ scenarios.yaml         # ScÃ©narios de performance
â”‚   â”œâ”€â”€ eval_tasks.yaml        # TÃ¢ches d'Ã©valuation
â”‚   â””â”€â”€ sampling_params.yaml   # ParamÃ¨tres d'Ã©chantillonnage
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lmstudio_client.py     # Client API LM Studio
â”‚   â”œâ”€â”€ checkpoint.py          # SystÃ¨me de checkpoint
â”‚   â”œâ”€â”€ reproducibility.py     # Seeds et environnement
â”‚   â”œâ”€â”€ statistics.py          # IC et tests statistiques
â”‚   â”œâ”€â”€ performance/           # Module performance
â”‚   â”‚   â”œâ”€â”€ runner.py          # Orchestrateur
â”‚   â”‚   â”œâ”€â”€ metrics.py         # Collecte mÃ©triques
â”‚   â”‚   â””â”€â”€ scenarios.py       # DÃ©finition scÃ©narios
â”‚   â”œâ”€â”€ capability/            # Module capacitÃ©s
â”‚   â”‚   â”œâ”€â”€ banking_eval.py    # Banking77 + PhraseBank
â”‚   â”‚   â”œâ”€â”€ coding_eval.py     # HumanEval
â”‚   â”‚   â”œâ”€â”€ harness_runner.py  # lm-evaluation-harness
â”‚   â”‚   â””â”€â”€ realistic_scenarios.py
â”‚   â””â”€â”€ compliance/            # Module conformitÃ©
â”‚       â”œâ”€â”€ risk_analysis.py   # NIST/OWASP
â”‚       â””â”€â”€ license_audit.py   # Audit licences
â”œâ”€â”€ prompts/                   # Templates de prompts
â”œâ”€â”€ scripts/                   # Points d'entrÃ©e
â”‚   â”œâ”€â”€ run_performance.py
â”‚   â”œâ”€â”€ run_capability.py
â”‚   â””â”€â”€ generate_report.py
â”œâ”€â”€ results/                   # RÃ©sultats gÃ©nÃ©rÃ©s
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ TUTORIAL.md               # Guide pas Ã  pas
```

---

## Citation

```bibtex
@software{edge_slm_benchmark,
  title = {Edge SLM Benchmark Framework},
  year = {2024},
  note = {Local Generative AI on Enterprise Edge Devices in Regulated Banking}
}
```

## Licence

Projet de recherche - Usage acadÃ©mique
